{
  "hash": "40894917ee89f261c851d4b122bbad1e",
  "result": {
    "markdown": "---\ntitle: \"Benefits of PCA\"\nauthor: \"paloma leonato\"\ndate: \"2023-11-19\"\ncategories: [R, Regression, analysis]\nimage: \"pic_anal.jpg\"\n---\n\n\nIt is advised to eliminate low-variance predictors and any constant predictors since those have less predictive power. Those could sometimes cause the model to crash due to divisions by zero. An alternative to removing the lo-variance predictors is to evaluate PCA in your dataset. Those low-variance predictors could be combined as a PCA variable with a significant impact on your model, tremendously affecting the accuracy of the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\ndata(mtcars)\n\nset.seed(1234)\n\nmtcars[sample(1:nrow(mtcars),10),\"drat\"] <- NA\n\nY <- mtcars$mpg\n\nX<- mtcars[,3:5]\n\nX$newx <- 2\n\n\nmodel0 <- train(\n  y = Y,\n  x = X, \n  method=\"glm\",\n  preProcess = c(\"nzv\",\"center\",\"scale\")\n)\n               \nsummary(model0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20.5028     0.6951  29.495   <2e-16 ***\ndisp         -1.3181     1.5577  -0.846   0.4085    \nhp           -2.6840     1.0764  -2.494   0.0226 *  \ndrat          1.6457     0.9471   1.738   0.0994 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 10.00881)\n\n    Null deviance: 726.22  on 21  degrees of freedom\nResidual deviance: 180.16  on 18  degrees of freedom\n  (10 observations deleted due to missingness)\nAIC: 118.69\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n\n```{.r .cell-code}\nmodel <- train(\n  y = Y,\n  x = X, \n  method=\"glm\",\n  preProcess = c(\"zv\",\"center\",\"scale\",\"pca\")\n)\n               \nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20.4127     0.6621  30.833  < 2e-16 ***\nPC1          -3.3087     0.4460  -7.419 5.04e-07 ***\nPC2           0.4893     0.8666   0.565    0.579    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 9.635294)\n\n    Null deviance: 726.22  on 21  degrees of freedom\nResidual deviance: 183.07  on 19  degrees of freedom\n  (10 observations deleted due to missingness)\nAIC: 117.05\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nThe PCA option will scale and center the data and then combine the low-variance variables in the preprocess argument, ensuring that all the predictors are orthogonal, reducing the risk of multicollinearity. This might improve the accuracy of your model\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "paloma leonato",
    "section": "",
    "text": "PALOMA LEONATO, MS, MBA, CSCP, PMP, CLTD\n214-202-5057 · pleonato@gmail.com\nHighly skilled Consulting leader, supply chain, finance, and cost management professional. Expert-level in supply chain, financial analysis, and costing with a proven background in business planning and process structures saving multiple millions of dollars. Specific areas of expertise include leading complex, IT/business transformation solutions, process assessment and improvement, strategic financial analysis, statistical model building, market research, cost analyses, evaluation of risk, and Financial Planning using statistical models.\n·        Master in Advance Data Analytics in process\n·        Led complex financial and cost analyses. Financial planning, forecast, analysis and benchmarking, optimization of financial operations and cost control, pricing strategy, product and customer profitability analysis. Requirements gathering, solution selection and IT implementation (ERP/SAP, ORACLE/NetSuite,CRM, SCM, BI/COGNOS, DOMO, TABLEAU, ARIBA).\n·        Trusted project management leader skilled at driving cross-functional teams to meet project and client objectives. ERP (SAP) transformation projects, application integration and SDLC. Business Intelligence IBM Cognos, Domo, Tableau Power BI. Led communications with Senior Leadership.\nAREAS OF EXPERTISE\nFinance, Costing and IT Expertise: Strategy, Controlling, OTC, STP, R2R, Product costing, IT infrastructure, S&OP, Benchmarking and Management reporting, GAAP, COSO Model, SOX, cash management and cost improvements, demand enhancements.\nProcess Improvement:  Deep understanding of SCOR model,Lean six-sigma DMAIC methodology, costing/forecasting, process mapping, profitability analysis tools, supply chains, operations planning.\nProject Management: Agile Methodology, Cross-functional team leadership, Six Sigma process improvement, Change management. PMO. ERP, CRM and Supply Chain Systems implementation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "paloma leonato",
    "section": "",
    "text": "CNN Optuna\n\n\n\n\n\n\n\nPython\n\n\nDL\n\n\nModel\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2025\n\n\npaloma leonato\n\n\n\n\n\n\n  \n\n\n\n\nBenefits of PCA\n\n\n\n\n\n\n\nR\n\n\nRegression\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\npaloma leonato\n\n\n\n\n\n\n  \n\n\n\n\nCross Validation\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncross validation\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\npaloma leonato\n\n\n\n\n\n\n  \n\n\n\n\nEvaluate your Model Quality\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\npaloma leonato\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\npaloma leonato\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Benefits of pca/index.html",
    "href": "posts/Benefits of pca/index.html",
    "title": "Benefits of PCA",
    "section": "",
    "text": "It is advised to eliminate low-variance predictors and any constant predictors since those have less predictive power. Those could sometimes cause the model to crash due to divisions by zero. An alternative to removing the lo-variance predictors is to evaluate PCA in your dataset. Those low-variance predictors could be combined as a PCA variable with a significant impact on your model, tremendously affecting the accuracy of the results.\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\ndata(mtcars)\n\nset.seed(1234)\n\nmtcars[sample(1:nrow(mtcars),10),\"drat\"] &lt;- NA\n\nY &lt;- mtcars$mpg\n\nX&lt;- mtcars[,3:5]\n\nX$newx &lt;- 2\n\n\nmodel0 &lt;- train(\n  y = Y,\n  x = X, \n  method=\"glm\",\n  preProcess = c(\"nzv\",\"center\",\"scale\")\n)\n               \nsummary(model0)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.5028     0.6951  29.495   &lt;2e-16 ***\ndisp         -1.3181     1.5577  -0.846   0.4085    \nhp           -2.6840     1.0764  -2.494   0.0226 *  \ndrat          1.6457     0.9471   1.738   0.0994 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 10.00881)\n\n    Null deviance: 726.22  on 21  degrees of freedom\nResidual deviance: 180.16  on 18  degrees of freedom\n  (10 observations deleted due to missingness)\nAIC: 118.69\n\nNumber of Fisher Scoring iterations: 2\n\nmodel &lt;- train(\n  y = Y,\n  x = X, \n  method=\"glm\",\n  preProcess = c(\"zv\",\"center\",\"scale\",\"pca\")\n)\n               \nsummary(model)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.4127     0.6621  30.833  &lt; 2e-16 ***\nPC1          -3.3087     0.4460  -7.419 5.04e-07 ***\nPC2           0.4893     0.8666   0.565    0.579    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 9.635294)\n\n    Null deviance: 726.22  on 21  degrees of freedom\nResidual deviance: 183.07  on 19  degrees of freedom\n  (10 observations deleted due to missingness)\nAIC: 117.05\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe PCA option will scale and center the data and then combine the low-variance variables in the preprocess argument, ensuring that all the predictors are orthogonal, reducing the risk of multicollinearity. This might improve the accuracy of your model"
  },
  {
    "objectID": "posts/crossvalidation/crossvalidation.html",
    "href": "posts/crossvalidation/crossvalidation.html",
    "title": "Cross Validation",
    "section": "",
    "text": "Hello today I am going to talk about cross validation. Cross validation will provide….\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nmydata &lt;- ISLR2::Portfolio\n\nset.seed(1234)\ncontrol_kfold &lt;- trainControl(method = \"cv\", number=10)\n\n\nkfold &lt;-train(Y~X , data = mydata, method = \"lm\", trControl = control_kfold)\n\nkfold$results\n\n  intercept      RMSE Rsquared       MAE    RMSESD RsquaredSD     MAESD\n1      TRUE 0.9544015 0.319766 0.7846862 0.2192737  0.2138706 0.1840715\n\n\nThis is not the best sample. But gets you the basics for the cross-validation. The training data is divided into 10 folds. Every time the model is created with 9 folds, one of the folds is left out to estimate the validation of the model. Once the model is created, you can validate your results with the model$results. In this case kfold$results."
  },
  {
    "objectID": "posts/crossvalidation/crossvalidation.html#cross-validation",
    "href": "posts/crossvalidation/crossvalidation.html#cross-validation",
    "title": "Cross Validation",
    "section": "",
    "text": "Hello today I am going to talk about cross validation. Cross validation will provide….\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nmydata &lt;- ISLR2::Portfolio\n\nset.seed(1234)\ncontrol_kfold &lt;- trainControl(method = \"cv\", number=10)\n\n\nkfold &lt;-train(Y~X , data = mydata, method = \"lm\", trControl = control_kfold)\n\nkfold$results\n\n  intercept      RMSE Rsquared       MAE    RMSESD RsquaredSD     MAESD\n1      TRUE 0.9544015 0.319766 0.7846862 0.2192737  0.2138706 0.1840715\n\n\nThis is not the best sample. But gets you the basics for the cross-validation. The training data is divided into 10 folds. Every time the model is created with 9 folds, one of the folds is left out to estimate the validation of the model. Once the model is created, you can validate your results with the model$results. In this case kfold$results."
  },
  {
    "objectID": "posts/my-first-post/index.html",
    "href": "posts/my-first-post/index.html",
    "title": "Post Estimation Diagnostics",
    "section": "",
    "text": "This is a post with executable code.\n\nlibrary(ISLR2)\n\nmydata &lt;- ISLR2::Portfolio\n\nmodel &lt;- lm(Y~X, data=mydata)\n\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.44650 -0.65599 -0.02448  0.53944  2.33072 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.05414    0.09877  -0.548    0.585    \nX            0.55497    0.09319   5.955 4.07e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9851 on 98 degrees of freedom\nMultiple R-squared:  0.2657,    Adjusted R-squared:  0.2582 \nF-statistic: 35.46 on 1 and 98 DF,  p-value: 4.068e-08\n\n\nWe can see that the X variable is significant. Notice the three start to the right.\nLet’s now plot the relation between these two variables.\n\nplot(mydata$X,mydata$Y)\n\n\n\n\nThere is a positive correlation between Y, the dependent variable, and X, the independent variable.\nNow let’s evaluate if the model is a valid model\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\npar(mfrow = c(2,2))\n\nplot(model)\n\n\n\n\nFirst, we call the library tidyverse which has this plot command to evaluate the feasibility of our model.\nThe top right graph shows that the residuals vs fitted values are randomly distributed, with a minimal deviation from the horizontal line.\nThe next graph to the right is the Q-Q plot. Q_Q plot checks the linearity of the data. If the points are all close to the line that is a good validation of the model.\nThe other two graphs will show and highlight the outliers and high-leverage points."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my Blog! In this blob I am sharing my learning experience with R and Python. I hope you will find it interesting."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Comparing Assets within Technology Adoption: Forecast Framework\n1. Core Objectives The primary goal of this project is to determine where emerging technologies specifically Artificial Intelligence (AI) and cryptocurrencies (decentralization) currently sit on the technology adoption lifecycle. The authors aim to forecast the potential prices of these assets, identify good market entry and exit points, and determine if macro-economic factors impact these nascent asset classes differently than traditional mature investments.\n2. Exploratory Data Analysis (EDA) and Market Insights The authors analyzed a dataset spanning from 2000 to 2025, comparing AI and cryptocurrencies against traditional stocks (like Apple), indexes, metals, and macroeconomic data (like M2 money supply and CPI). Key findings from the EDA include:\n\nTechnology Adoption Phase: Cryptocurrencies are estimated to be starting at the steepest point of the adoption S-curve, with an estimated eight years of exceptional returns remaining before maturity. AI is estimated to be at 35% adoption (the “early majority” phase), with about five years of rapid adoption remaining.\n\nMacroeconomic Impact: Unlike mature assets (like traditional stocks), pure-play AI (e.g., NVIDIA) and cryptocurrencies have low or negative correlations to macroeconomic variables like inflation (CPI) and money supply. Their prices are currently driven by rapid adoption rather than macro-economy health.\n\n\n3. Forecasting Framework and Modeling The project evaluates various deep learning architectures (Fully Connected, RNN, LSTM, GRU, CNN, and their bidirectional variants) to forecast Apple stock prices as a baseline model that could eventually be applied to other assets. The forecasting was divided into three time horizons using a sliding window approach:\n\nLong-term: 1-year windows predicting 1 year ahead (using data from 2009 onward).\nMedium-term: 120-day windows predicting 6 months ahead (using data from 2014 onwar)\nShort-term: 15-day windows predicting 3 weeks ahead (using data from 2020 onward).\n\n4. Key Model Results The models were benchmarked against a Naïve forecasting method.\n\nSimpler models (like standard RNN, GRU, and Bidirectional RNN/GRU) consistently outperformed highly complex models like LSTM and Bidirectional LSTM, which proved slow, difficult to tune, and prone to overfitting.\nFor multivariable long-term forecasting, reducing dimensionality using Principal Component Analysis (PCA) successfully mitigated multicollinearity issues and improved model performance.\nRecommending carefully matching the model architecture and preprocessing strategy to the specific forecasting window is more important than simply utilizing the most complex neural network.\n\n\n\nNeural Network Recession Prediction Indicator:\nEconomic cycles affect the financial well-being of the US and the world population. A downturn can negatively impact the populace. Inflation causes purchasing power to decrease. When unemployment rises, people lose their ability to meet their financial borrowers are unable to stay current.\nThis is the ideal time for us to create a model to predict the future state of the economy. Economic anomalies exist that make this hard to predict. There are geopolitical factors that have influence over the future of the economy, including the elections in the United States. There is also a wide range of opinions from economists.\nMachine learning (ML) models are increasingly replacing traditional statistical models due to their ability to analyze vast amounts of data and multiple variables, resulting in more reliable outcomes.\n\n\n\n\n\nOur objective is to predict if a recession is going to happen in the next six months. We have four scenarios that we define for our target variable:no recession in the next six months, mild recession will begin in the next six months, moderate recession will start in the next six months and will last between six and twelve months,and deep recession will start in the next six months. This will last between twelve and thirty-six months.\nWe assess the normalization of the final 43 selected variables.\n\n\n\n\n\nWe consider it critical to have a split records in training and testing where we keep the distribution of the classes. For that, we imported the train_test_split method from the sklearn.model_selection library. This ensures that the distribution of the training and test is the same. We believe that this split is critical to design a robust model. In the above figure, we can see the distribution of the class for the test dataset, in yellow, after the split. We can see the same distribution in a reduced number of records for the test dataset.\n\n\n\n\n\nThere are three possible models that we could use fully connected (FC), convolutional (CNN), and recurrent neural network (RNN) , even though we understand that recurrent neural networks are the most suitable for serial data, we didn’t want to limit our study to one model and work with an approach that evaluated all possible alternatives.\n\n\n\n\n\nWith the CNN, we proceeded with further optimization using maxpooling and different no-saturated activation functions like ReLU and LeakyReLU. We use max-pooling because, during our data exploration, we identify extreme values that might be captured in max-pooling. This layer also reduces the dimension, improving performance and lowering cost. The best results were with the leakyReLU activation function and maxpooling with 99% F1 score.\nIn this study, we developed a comprehensive machine learning model to predict the likelihood and severity of upcoming recessions. We selected this thesis due to the uncertainty of the current economic environment. Using a 36-month look-back period and 43 economic indicators, our model offers a significant advancement over existing predictive methodologies.\nWe evaluated FC, CNN, RNN and LSTM models and different window sizes with different slides: 30-day window sizes with a 10-day slide to predict the recession index for the 30th day. Then only for the RNN and LSTM we evaluate the following cases: a 30-day window with a 10-day slide to predict the recession index for the 60th day and a 60-day window with a 30-day slide to predict the recession index for the 90th day. For each neural network model, We developed a simple base model, tried every scheduler and optimizer and selected the best result, and with those, we ran Optuna to find the best hyperparameters with 75 trials. In the following graph we are just showing the Optuna model for all the different combinations.\n\n\n\n\n\n\n\n\n\n\nAfter evaluating all the models, we identified the need for additional data to develop a model that has a higher number of rolling days and can predict future recession indexes. We got the best result with the CNN, but this one requires a longer processing time, which will increase cost. RNN and LSTM are the best alternatives; although both drop the F1 scores with a limited number of training data, the stability of these models is remarkable. Also, we are concerned about overfitting, and we recommend early stopping and exploring data augmentation with synthetic data generation. There are two possible techniques: SMOTE (Synthetic Minority Over-sampling Technique) and GANs (Generative Adversarial Networks), to create synthetic data that mimics the characteristics of the historical records. The alternative will be to go back in history, although we are concerned that some of the variables might not be available if we go back too many years.\n\n\nFactors that Influence Health: US Life Expectancy:\nLife expectancy is a vital indicator that provides insight regarding a population’s overall health, and it serves as one of the core indicators of calculating the United Nations Human Development Index across countries. According to the World Health Organization data, life expectancy at birth in the United States is 78.5 years for both sexes in 2019; but when compared to other high-income countries, the United States’s life expectancy measured to be 2.4 years lower and ranked 40th in the world (World Health Organization, 2019).\n\n\n\n\n\n\n\n\n\n\nWith life expectancy being a primary indicator for a population’s overall health, this project aims to determine how lifestyle and socio-economic factors have impacted life expectancy and its disparities among counties (minimum geographical location) within the United States between the years of 2019 to 2023.\n\n\n\n\n\nThe data used in our study revealed the impact of location on life expectancy. The South region had poor lifestyle behaviors, as we observed from the graphs. As we learned from the literature review, physical and social environments in which people live helped determine the quality of life and the life expectancy. The South region was more rural, less educated, and had the highest percentage of poverty. Comparatively, the West region had the highest life expectancy and was on the other side of the spectrum of the South region for the variables in the study. We also observed during the exploration of the data the South region is the region with the lowest life expectancy with an average of 76.07, while the West region had an average life expectancy about three years higher, in the range of 79.15.\n\n\n\n\n\n\n\n\n\n\nWhile evaluating the validity of the model, we observed that the residuals were randomly distributed. However, in the QQ plot, we observed tails with high curvature, specifically to the right, which reveals the presence of extreme outliers.\n\n\n\n\n\nThe Residuals vs Leverage graph did not show the presence of any high leverage points. We calculated the Full Model and best subset model, eliminating the outliers, and assessed the validity of the model again.\nThe R-squared and adjusted R-squared improve from 0.6851 to 0.7798\nLasso approach used a shrinkage technique that would significantly reduce the variance. Lasso eliminated the same variables that were found in the stepwise approach. Evaluating the MSPE and comparing it with the cross-validation ten-fold for both full model and bestsubset model, we saw that Lasso provided a lower MSPE. \n\n\n\n\n\nTo evaluate the health factor that contributes the most to life expectancy, we picked the Lasso model because it has the lowest Error MSPE = 3.027 and is the model that predicts the average life expectancy in the US more accurately. \n\n\n\n\n\n\n\nClassification of potential Donors and prediction of Gift Amount:\nbegin{left}We have been approached by a non-profit organization to build a classification and predictive model that will be used to increase the cost-effectiveness of their current direct marketing campaign. The organization has provided data with a historical background to its direct marketing campaign. Personalized address mailers are sent out to previous donors for $2.00; the average donation gift amount is $14.50 from respondents. However, the campaign is operating at a loss due to low donation rates of 10%. The organization would like for us to take a provided dataset of prior donors to create our analytical models in order to make their direct marketing campaign not operate at a loss.\n\n\n\n\n\nWe assess the potential profit of our study by using the adjusted confusion matrix from the ensemble classification model with the average donation amount from the AutoNeural validation model to calculate various profit-losses, considering with and without opportunity costs. We will then use the percentages of the adjusted confusion matrix to estimate the profit or loss generated by the score data results. We will also consider the lift provided by the score data.\nThe resulting profit from our model, based on the validation classification data with the score donation value, was $1,695.65. We ignore the opportunity cost from false negatives in our profit evaluation.\n\n\nForecast model to predict the demand of Covit-19 vaccine\nAccording to the World Health Organization, Covid-19 is an infectious disease that cause symptoms such as fever, cough, tiredness, and loss of taste or smell. The World Health Organization stated that anyone can get sick with the virus and become extremely sick or die at any age. Covid-19 outbreak goes back to late December 2019 in Wuhan, Hubei, China as mentioned by (Wu et al., 2020). Moreover, (Ybarra, 2022) added that some Covid-19 cases suffer from something called long-term Covid which means some Covid-19 symptoms affect parts of the body such as the lungs, causing trouble in breathing, harsh cough, and shortness of breath. More than a year ago, a vaccine was derived against the Covid-19 virus. 4.4 billion people have had one or more doses, which represents 56% of the world’s population (Mallapaty et al., 2021). In this project, we will study the vaccination process in California and try to predict the booster-eligible population in all California counties.\n\n\n\n\n\nI worked on a model to calculate the demand for vaccines in the state of California. Two models were presented aggregating the data in monthly buckets with 12-month lags as independent variables. In the second model, the data was aggregated in weekly buckets with 4-week lags as independent variables. The data was split into two datasets without breaking the sequence of the dataset, training with 65% of the data and testing with 35% of the remaining values in the dataset. The model was trained using a linear regression algorithm.\n\n\n\n\n\nI recommended the model with weekly buckets to provide accurate predictions of future demand.\n\n\nSalaries, Foreign born vs Native in Texas.\nIn this project, we evaluate the attributes that influence a worker’s salary in Texas. We proceed with analytics of the data including data exploration and data cleansing.\n\n\n\n\n\nDeep understanding of SCOR model, Lean six-sigma DMAIC methodology, costing/forecasting, process mapping, profitability analysis tools, supply chains, and operations planning. The main attributes evaluated were place of birth, jobs, occupations, income,sex, and educational attainment. For this project we used Google Big Query, Python linear regression, and decision tree in SAS.\n\n\n\n\n\nThe best salary among the four males was predicted for a foreign male with a graduate degree who runs his own business."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "resume",
    "section": "",
    "text": "214-202-5057 · pleonato@gmail.com\nHighly skilled Consulting leader, supply chain, finance, and cost management professional. Expert-level in supply chain, financial analysis, and costing with a proven background in business planning and process structures saving multiple millions of dollars. Specific areas of expertise include leading complex, IT/business transformation solutions, process assessment and improvement, strategic financial analysis, statistical model building, market research, cost analyses, evaluation of risk, and Financial Planning using statistical models.\nMaster in Advance Data Analytics in process\nLed complex financial and cost analyses. Financial planning, forecast, analysis and benchmarking, optimization of financial operations and cost control, pricing strategy, product and customer profitability analysis. Requirements gathering, solution selection and IT implementation (ERP/SAP, ORACLE/NetSuite,CRM, SCM, BI/COGNOS, DOMO, TABLEAU, ARIBA).\nTrusted project management leader skilled at driving cross-functional teams to meet project and client objectives. ERP (SAP) transformation projects, application integration and SDLC. Business Intelligence IBM Cognos, Domo, Tableau Power BI. Led communications with Senior Leadership."
  },
  {
    "objectID": "resume.html#paloma-leonato-ms-mba-cscp-pmp-cltd",
    "href": "resume.html#paloma-leonato-ms-mba-cscp-pmp-cltd",
    "title": "resume",
    "section": "",
    "text": "214-202-5057 · pleonato@gmail.com\nHighly skilled Consulting leader, supply chain, finance, and cost management professional. Expert-level in supply chain, financial analysis, and costing with a proven background in business planning and process structures saving multiple millions of dollars. Specific areas of expertise include leading complex, IT/business transformation solutions, process assessment and improvement, strategic financial analysis, statistical model building, market research, cost analyses, evaluation of risk, and Financial Planning using statistical models.\nMaster in Advance Data Analytics in process\nLed complex financial and cost analyses. Financial planning, forecast, analysis and benchmarking, optimization of financial operations and cost control, pricing strategy, product and customer profitability analysis. Requirements gathering, solution selection and IT implementation (ERP/SAP, ORACLE/NetSuite,CRM, SCM, BI/COGNOS, DOMO, TABLEAU, ARIBA).\nTrusted project management leader skilled at driving cross-functional teams to meet project and client objectives. ERP (SAP) transformation projects, application integration and SDLC. Business Intelligence IBM Cognos, Domo, Tableau Power BI. Led communications with Senior Leadership."
  },
  {
    "objectID": "resume.html#areas-of-expertise",
    "href": "resume.html#areas-of-expertise",
    "title": "resume",
    "section": "AREAS OF EXPERTISE",
    "text": "AREAS OF EXPERTISE\n\nFinance, Costing and IT Expertise:\nStrategy, Controlling, OTC, STP, R2R, Product costing, IT infrastructure, S&OP, Benchmarking and Management reporting, GAAP, COSO Model, SOX, cash management and cost improvements, demand enhancements.\n\n\nProcess Improvement:\nDeep understanding of SCOR model,Lean six-sigma DMAIC methodology, costing/forecasting, process mapping, profitability analysis tools, supply chains, and operations planning.\n\n\nProject Management:\nAgile Methodology, Cross-functional team leadership, Six Sigma process improvement, Change management. PMO. ERP, CRM, and Supply Chain Systems implementation."
  },
  {
    "objectID": "resume.html#professional-synopsis",
    "href": "resume.html#professional-synopsis",
    "title": "resume",
    "section": "PROFESSIONAL SYNOPSIS",
    "text": "PROFESSIONAL SYNOPSIS\nArcosa November 2018-2024\nDirector Supply Chain, Manufacturing, and Engineering Solutions\n\nSuccessfully support all supply chain, manufacturing, and engineering applications during and after the migration from Trinity. Successfully stabilized support of portfolio applications with a brand new supporting team.\nSuccessfully led several supply chain process improvement projects through different business units in ARCOSA. (ARIBA, OTM, S&OP, EDI, Barcoding among others)\nLed the development of new enhancements and mobile applications to bring efficiencies into the ARCOSA supply chain. Serialized physical inventory and Shipping module mobile applications.\nLed the digital transformation for Arcosa. Led the next generation ERP solution implementation for four business units in ARCOSA. Successfully go live with two new next generation ERP, virtually led technical and functional team. Led the design and successful development of the ERP integration with the corporate financial systems.\nLed the process documentation and worked with internal audit to set controls and mitigation plans. Got consensus with leadership from different Business Units to implement a solution to facilitate SOX compliance, security and audit activities.\n\nTrinity Industries 2017- November 2018\nSenior Project Management\n\nLed strategic initiatives for the Supply Chain and Manufacturing group.\nImplement the first mobile solution at Trinity and Arcosa. Reporting production, physical inventory Purchase, transfer, and resupply orders receipts.\nLed the successful migration of all supply chain, manufacturing and engineering applications from Trinity to Arcosa.\n\nPremier Trailer Leasing 2016-2017\nSenior Director – Project Management\nSuccessfully managed strategic corporate initiatives resulting in significant improvements in the Order to Cash process. Defined a dashboard to monitor processes.\n\nLed a major implementation of a billing solution. Changed the solution approach and integration with ERP resulting in significant savings on total cost of ownership (TCO) and reduction of invoicing errors. Successfully implemented the solution based on a new project plan on time and budget. Skillfully drove leadership to consensus and set the change management plan to successfully adopt the change internally ( by 33 branches) and externally ( by customers). Defined baseline KPI and reporting requirements to monitor and improve the new process.\nSuccessfully led the implementation of a cash app, resulting in a 30% of labor cost reduction and a significant reduction of “on account” funds. Successfully implemented the electronic billing of key national customers, integrating the solution with ARIBA, OB10, and Taulia. This resulted in a significant increase in business and a reduced Cash to Cash cycle.\nManaged the development and rollout of a mobile solution to outbound and inbound and automatically created repair work orders for leasing units. The solution resulted in a 50% reduction in labor costs.\nSuccessfully re-engineered the collection solution, resulting in a 15% reduction of past-due items. Integrated the solution with the CRM, increasing customer touch point visibility/relations and shortening payment cycles while increasing customer satisfaction.\n\nSCM@RISK, Plano, Texas · 2012- 2016\nIT , Supply Chain and Costing Consulting Services.\nSupply Chain and Finance Optimization Principal\nLocal independent consulting services in balancing IT, Supply Chain cost efficiency, optimization and customer service.\n\nLed major IT/Business efficiency initiatives (Lean Six Sigma) in a fortune 500 company in the Health Care industry. Implemented a new financial control process and effectively managed the first project on budget for the group. Achieved a strong record of success in a dynamic environment. Improved onboarding process by 40%. Led the technical and business team to successfully integrate the solution with the CRM Salesforce.  Use Clarity and SAP Project Systems to plan and monitor Cost.\nLeading operation, cost control and improvements initiatives in a leader telecommunication company.\nSuccessfully developed, implemented, and managed complex projects within time and budgetary constraints. Designed and implemented KPI to monitor continuous improvement initiatives. Using SAP BI. Manage Financial and Cost reporting to leadership.\n\nAPICS North Texas · 2007-Present\nNon-Profit Supply Chain education organization\nPresident 2012-2014, Platinum Performance Achievement Award., currently member Board of Directors:\nHolding a board of director’s seat for over 8 years.\nOverseeing and coordinating all aspects of budgeting and financial management.\n\nDeveloped and implemented innovative programs and services. Conducted ongoing output and cost-benefit analysis for all programs implemented. Responsible for completing audit fieldwork, which included dealing with accounting staff. Performed due diligence and cost certifications.\nBuilt and retained exceptional staffs and created excellent work environments. established a volunteer incentive program tied to the group performance indicators.\nLeading organizations and departments through periods of substantial growth and transition.\nStrategic planning and facilitation. Led various strategic planning processes for the local organizations. Well-versed in translating missions and goals into operating plans with clear benchmarks and annual objectives.\n\nINFOSYS, Plano, Texas · 2010- 2012\nGlobal leader in the “next generation” of IT and consulting company.\nSupply Chain Management Process Improvement Principal:\n\nSeasoned IT Supply Chain management and costing professional with extensive functional skills in product costing, financial planning, controller organizations, and Supply Chain Management.\nWorked on Business Development activities. Develop Costing Models to price RFP using internal Costing tools. Calculated Total Cost of ownership (TCO) to support IT leadership Decisions.\nDeveloped detailed change management strategy and communication plan, with identification of resources, roles, and responsibilities for a large supply chain transformation project for a global leader in the transportation industry.  Including the detailed organizational impacts and roadmap for change actions. Led process integration team during the design phase for a global leader in the consumer product industry.\n\nNORTH HIGHLAND, Dallas, Texas · 2006-2009\nInternational management and technology consulting company.\nSupply Chain and Finance Sr. Manager:\n\nDirect the development of costing and reporting structure improvements for Fortune 500 SAP environments. Oversee all development strategies to improve the variance analysis and purchasing process.\nLed the full cycle conversion of Fixed Asset capital expenditure and retirement data from FAS tax system and implemented in the SAP Fixed asset module. This included configuring SAP depreciation keys, screen layouts, and depreciation areas and creating tax adjustment transaction types for tax book requirements. Led design development, testing, and cutover plan to move into production. Used Cognos, and Access for reconciliation. \nDelivered $1M in savings by leading the restructuring of customer/sourcing service processes. Conducted detailed expense analysis using Cognos Reporting, Access and Excel for analytics.\nLed Lean Six Sigma product costing improvement. Led the SAP design improvement and training implementation. Decreased direct manufacturing and OH allocations variance by over 40% and PPV by over 30% by directing the redesign of cost improvement, variance analysis, and purchasing processes.\nSaved over $15M by spearheading the design and launch of a profitability analysis tool that identified and corrected the client’s internal product cost control issues. Implemented internal controls for compliance.\nChampioned the design of a supply chain assessment application that improved supply chain performance by leading all integration activities. Designed complex reporting and Dashboard metrics with IBM Cognos.\nReceived Delivery Excellence award and Local Leader Champion diversity award. Promoted to Sr Mgr. Jan/09.\n\nCAPGEMINI, Dallas, Texas · 2004-2006\nGlobal consulting, technology, outsourcing and local professional services firm.\nSupply Chain Management Manager:\n\nOversaw all strategic forecasting and supply chain management development efforts. Served as supply chain and package solution expert for business development project cycles. Mentored and partnered with team members to achieve project goals.\nContributed a key role in securing a $10M+ business deal by working as a supply chain/package solution expert during new business development projects. Developed Costing Models using Excel.\nSaved $1M annually by designing and implementing a new sales and operations planning process.\nImproved forecasting accuracy to 89%, lowered stock-out by 30%, and decreased supply chain costs by 2% by developing a forecasting methodology integrating supply chain, merchandising, and suppliers into the forecasting. Defined and implemented key supply chain metrics in Cognos to monitor improvement progress. Report project financials and monitor cost models with Clarity and Excel.\nLowered launch time by 10% by coaching and mentoring 10 team members in implementation practices.\n\nJDA/MANUGISTICS, Dallas, Texas · 2000-2004\nGlobal provider of supply chain management and pricing and revenue optimization software.\nSupply Chain Advanced Optimization Project Manager::\n\nLed all client and internal projects. Collaborated with teams to create integrated supply chain management tools to enhance client business operations. Worked with client product management, marketing, distribution, logistics, and supply chain management teams to develop processes aligned with company business objectives. Partnered with client teams to design planning forecasts, vendor management inventory, and replenishment procedures. Directed integrated supply chain performance management projects. Developed processes that combined ERP and SCM analytics to create better integrations.\nReduced inventory/carrying costs, improved sales service levels/inventory turns for clients, and improved plant capacity and distribution planning processes among client’s multi-tiered network.\nCreated balanced scorecards and defined KPIs in Cognos BI to monitor supply chain improvements.\nDelivery Excellence Project and Project of the Year award recipient.\n\nEDS – A. T. KEARNEY, Plano, Texas · 1996-2000\nGlobal information technology industry provider addressing clients’ critical business needs with IT solutions.\nSenior SAP Consultant:\n\nDrove FI / CO team efforts in SAP technology projects’ design and development processes. Oversaw system testing, requirements identification, design specs, and all training and support activities. Managed all cross-functional project team processes. Directed full lifecycle implementations.\nReceived certification in SAP FI/CO and ASAP.\n\nEDS Accounting and Finance Development Leadership Program Member:\nHired into a prestigious leadership program to participate in international finance, operations, strategic sourcing, and costing projects. Learned and conducted a comprehensive analysis cost structure, focusing on corporate/unit overhead.\n\nSaved $6M annually by recommending activity-based costing methods (service level rationalization) to the CIO.\nGraduated with honors from the program."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "resume",
    "section": "EDUCATION",
    "text": "EDUCATION\nMaster of Business Administration in International Corporate Finance and Management\nThunderbird School of Global Management, Glendale, Arizona\nMaster of Data Science and Advanced Analytics (Cum GPA 4.0)\nUniversity of North Texas, Denton\nMaster of Science in Power Generation\nPolytechnic University of Madrid, Madrid, Spain\nBachelor of Science in Engineering\nPolytechnic University of Madrid, Madrid, Spain\nMaster of Marketing and Business Management in Marketing of Industrial Products\nESIC, Madrid, Spain\nPROFESSIONAL DEVELOPMENT\nMaster in Advanced Data Analytics – UNT in process\nPMI Project Management Institute, PMP. CMA Certified Management Accountant- IMA\nAPICS Instructor for Supply Chain Management Certification, CSCP and Production and Inventory Management Certification, CPIM. Certified in Logistics, Transportation and Distribution. CLTD\nStatistical Modeling and analysis using Monte Carlo simulation  @RISK . RStudio\nBig Data and Data Science training at Udacity (Python, Panda)\nSearch engine Optimization Certification\nExpert in all Microsoft product including Access and Visio\nTeaching Assistant: Thunderbird School of Global Management\nCOGNOS certified · ORACLE SQL Certified · MS Word/Excel/PowerPoint/Access/Visio/Project\nCSCP Certified Supply Chain Professional – APICS · Project Management · Business Process Design\nSAP  FI/CO certification  (Partner Academy) · Lean Six Sigma Green Belt certified\nASAP certification · JDA/Manugistics · Sterling Commerce · PROSCI Change Management certification\n ORACLE Database certification · UNIX SCO certification\nAFFILIATIONS\nIMA The Association of Accountants and Financial professionals in Business.\nAPICS Association of Operations Management- Former President NTX APICS Chapter – CSCP Instructor\nVoluntary Interindustry Commerce Standards Association (VICS) · United States Tennis Association\nUnited States Equestrian Federation Association · Thunderbird School of Global Management Alumni Association  · Dallas Business Club Member"
  },
  {
    "objectID": "posts/DL Optuna for CNN/index.html",
    "href": "posts/DL Optuna for CNN/index.html",
    "title": "CNN Optuna",
    "section": "",
    "text": "Optuna is a powerful open-source framework that automates the process of finding the best hyperparameters for deep learning (DL) models. By intelligently sampling and evaluating different sets of hyperparameters, Optuna identifies configurations that maximize model performance (e.g., accuracy, F1-score) or minimize loss. This iterative process significantly reduces the time and effort required to manually tune hyperparameters, leading to improved model performance and faster development cycles.\n\n\n\n\n\n\n\n\n\n\nEfficiency: Optuna’s intelligent search algorithms efficiently explore the hyperparameter space, finding optimal configurations faster than manual grid search or random search.\nFlexibility: Supports a wide range of hyperparameters, including learning rate, batch size, number of layers, activation functions, and more.\nScalability: Can be easily parallelized and distributed across multiple machines, accelerating the optimization process for computationally expensive models.\nEase of use: Provides a user-friendly API with clear documentation and examples, making it easy to integrate into existing DL workflows.\nBy leveraging Optuna, researchers and practitioners can significantly improve the performance of their DL models, leading to more accurate predictions, faster training times, and ultimately, better outcomes in their applications.\nThe PCA option will scale and center the data and then combine the low-variance variables in the preprocess argument, ensuring that all the predictors are orthogonal, reducing the risk of multicollinearity. This might improve the accuracy of your model"
  },
  {
    "objectID": "CNN_New_NoIdl.html",
    "href": "CNN_New_NoIdl.html",
    "title": "Import Libraries and required classes and Methods from Idlman",
    "section": "",
    "text": "# TEAM RL\n# This code is written to import the packages and tqdm notebook.\n# This includes tqdm, numpy, seaborn, matplotlib, pandas and time.\n\n\nfrom tqdm.autonotebook import tqdm\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\n\nimport time\n\nTqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n#TEAM RL\n#This code imports the relevant torch packages\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import *\n# from idlmam import *\n#TEAM RL\n# This code is written to import the packages and tqdm notebook.\n# This includes tqdm, numpy, seaborn, matplotlib, pandas and time.\n# This code imports pytorch.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom tqdm.autonotebook import tqdm\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\n\nimport time\n\nfrom idlmam import train_simple_network, Flatten, weight_reset, set_seed, run_epoch, train_network"
  },
  {
    "objectID": "CNN_New_NoIdl.html#load-data-and-data-exploration",
    "href": "CNN_New_NoIdl.html#load-data-and-data-exploration",
    "title": "Import Libraries and required classes and Methods from Idlman",
    "section": "Load data and data Exploration",
    "text": "Load data and data Exploration\n\n#TEAM RL\n# mount the driver to be able to load data\n\nfrom google.colab import drive\n\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n#TEAM RL Load the data.\n#load the Recession index with values only data first date of the Q\n\ndata = pd.read_csv(\"/content/drive/MyDrive/Project_5550/data_reduced_cleaned.csv\")\n\n\n#TEAM RL\n#This code shows the headers to insure we have the right data\n\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\ndate\nX2Yr\nX30Yr\nX2yr.10yr\nUnemployment\nGDP\nX3MUnemployment\nX6M1Mo\nX6M5Yr\nX6M30Yr\n...\nX24MCPI\nX24MM2\nX36M1Yr\nX36M7Yr\nX36M30Yr\nX36M2yr.10yr\nX36MUnemployment\nX36MSoftDev\nX36MM2\nY\n\n\n\n\n0\n1997-01-26\n1.94\n0.0\n2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n1\n1997-01-27\n1.94\n0.0\n-2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n2\n1997-01-28\n1.94\n0.0\n-2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n3\n1997-01-29\n1.94\n0.0\n-2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n4\n1997-01-30\n1.94\n0.0\n-2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n\n\n\n5 rows × 45 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n#TEAM RL\n#This code converts the date field into a datetime\ndata[\"date\"] = pd.to_datetime(data[\"date\"])\n\n\n#TEAM RL\n#This code displays the data types\ndata.dtypes\n\n\n#TEAM RL\n#This code displays graphs for the second 10 variables and the Y variable\ndata.set_index('date')[['X18M1Mo', 'X18M5Yr', 'X18M30Yr', 'X18MSAHM', 'X18MUnemployment', 'X18MSoftDev', 'X18MM2', 'X18MGDP', 'X24M1Mo', 'Y']].plot(subplots=True)\n\narray([&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;], dtype=object)\n\n\n\n\n\n\n##TEAM RL\n#This code displays graphs for the third 10 variables and the Y variable\ndata.set_index('date')[['X24M5Yr', 'X24M30Yr', 'X24M2yr.10yr', 'X24MSAHM', 'X24MUnemployment', 'X24MSoftDev', 'X24MCPI', 'X24MM2', 'X36M1Yr', 'Y']].plot(subplots=True)\n\narray([&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;], dtype=object)\n\n\n\n\n\n\n#TEAM RL\n#This code displays graphs for the last set of variables and the Y variable\ndata.set_index('date')[['X36M7Yr', 'X36M30Yr', 'X36M2yr.10yr', 'X36MUnemployment', 'X36MSoftDev', 'X36MM2', 'Y']].plot(subplots=True)\n\narray([&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;,\n       &lt;Axes: xlabel='date'&gt;], dtype=object)\n\n\n\n\n\n\n#TEAM RL\n#This code sets the 'date2' field as an index.\n\ndata.set_index('date', inplace=True)\n\n\n#TEAM RL\n#This code shows the headers to see if the data came in as expected\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nX2Yr\nX30Yr\nX2yr.10yr\nUnemployment\nGDP\nX3MUnemployment\nX6M1Mo\nX6M5Yr\nX6M30Yr\nX6MSAHM\n...\nX24MCPI\nX24MM2\nX36M1Yr\nX36M7Yr\nX36M30Yr\nX36M2yr.10yr\nX36MUnemployment\nX36MSoftDev\nX36MM2\nY\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1997-01-26\n1.94\n0.0\n2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n0.03\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n1997-01-27\n1.94\n0.0\n-2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n0.03\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n1997-01-28\n1.94\n0.0\n-2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n0.03\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n1997-01-29\n1.94\n0.0\n-2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n0.03\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n1997-01-30\n1.94\n0.0\n-2.44\n5.2\n0.0\n4.9\n0.88\n3.36\n0.0\n0.03\n...\n164.7\n4425.3\n1.31\n3.9\n0.0\n-2.44\n4.1\n99.595302\n4679.4\n2\n\n\n\n\n\n5 rows × 44 columns"
  },
  {
    "objectID": "CNN_New_NoIdl.html#prepare-the-data-scaling-and-create-the-sliding-windows.",
    "href": "CNN_New_NoIdl.html#prepare-the-data-scaling-and-create-the-sliding-windows.",
    "title": "Import Libraries and required classes and Methods from Idlman",
    "section": "Prepare the data scaling and create the sliding windows.",
    "text": "Prepare the data scaling and create the sliding windows.\n\n#TEAM RL\n#This code scales the non date columns in order to process them in the fully connected network.\n\ncol_to_scale = []\nfor col in data.columns:\n    if col != 'Y':\n        col_to_scale.append(col)\nprint(col_to_scale)\n\n['X2Yr', 'X30Yr', 'X2yr.10yr', 'Unemployment', 'GDP', 'X3MUnemployment', 'X6M1Mo', 'X6M5Yr', 'X6M30Yr', 'X6MSAHM', 'X6MCPI', 'X6MM2', 'X12M1Mo', 'X12M7Yr', 'X12M30Yr', 'X12MSAHM', 'X12MUnemployment', 'X12MM2', 'X12MGDP', 'X18M1Mo', 'X18M5Yr', 'X18M30Yr', 'X18MSAHM', 'X18MUnemployment', 'X18MSoftDev', 'X18MM2', 'X18MGDP', 'X24M1Mo', 'X24M5Yr', 'X24M30Yr', 'X24M2yr.10yr', 'X24MSAHM', 'X24MUnemployment', 'X24MSoftDev', 'X24MCPI', 'X24MM2', 'X36M1Yr', 'X36M7Yr', 'X36M30Yr', 'X36M2yr.10yr', 'X36MUnemployment', 'X36MSoftDev', 'X36MM2']\n\n\n\n#TEAM RL\n#This code displays the data types to insure they are formatted for the model to accept.\ndata.dtypes\n\n\n#TEAM RL\n#This code defines a sliding window as follows:\n\n#SlidingWindow(df, w, s, lag)\n#Four Arguments:\n# df: Is the DataFrame being processed.\n# w: Represents the window size, the number of rows or time steps included in each sliding window.\n# s: Represents the stride (step size), determining how far the window moves forward each time.\n# lag: Represents the lag parameter, indicating how much earlier the data should start in relation to the current time step.\ndef SlidingWindow(df,w,s,lag):\n    X=[]\n    Y=[]\n    for i in range(0,len(df)-lag*w,s):\n        x = np.array(df.iloc[i:i+w,:43])\n        y = np.array(df.iloc[i+w-1,-1])\n        X.append(x)\n        Y.append(y)\n\n    X=np.array(X)\n    Y=np.array(Y)\n\n    return X,Y\n\n\n#TEAM RL\n#This code defines a sliding window as follows:\n\n#SlidingWindow(df, w, s, lag)\n#Four Arguments:\n# df: Is the DataFrame being processed.\n# w: Represents the window size, the number of rows or time steps included in each sliding window.\n# s: Represents the stride (step size), determining how far the window moves forward each time.\n# lag: Represents the lag parameter, indicating how much earlier the data should start in relation to the current time step.\n\nw=30\ns=10\nlag=1\n\nX, y = SlidingWindow(data,w,s,lag)\n\n\n#TEAM RL\n# THis code identifies the shape of the data\nX.shape\n\n(996, 30, 43)\n\n\n\n#TEAM RL\n# This code identifies the y shape of the data\ny.shape\n\n(996,)\n\n\n\n#Team RL\n#This code identifies the Y\ny[0]\n\n2\n\n\n\n#TEAM RL\n#This code imports the MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nclass MinMaxScaler3D(MinMaxScaler):\n\n    def fit_transform(self, X, y=None):\n        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))\n        return np.reshape(super().fit_transform(x, y=y), newshape=X.shape)\n\n\n#Team RL\n#This code defines the scaler and X\nscaler = MinMaxScaler3D()\nX = scaler.fit_transform(X)\n\n\n#TEAM RL\n#THis block defines the shape of X\nX.shape\n\n(996, 30, 43)\n\n\n\n#TEAM RL\n#This block imports the train test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n#X_train, y_train = X[:796], y[:796]\n#X_test, y_test = X[796:], y[796:]\n\n\n#TEAM RL\n#This code displays the sapes of the data\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((796, 30, 43), (200, 30, 43), (796,), (200,))\n\n\n\n# TEAM RL\n# Get unique values and their counts\nunique, counts = np.unique(y_train, return_counts=True)\n\n# Create a dictionary to see the distribution of each class\ndis_train = dict(zip(unique, counts))\nprint(dis_train)\n\n{0: 649, 1: 7, 2: 96, 3: 44}\n\n\n\n#TEAM RL\n# Get unique values and their counts\nunique, counts = np.unique(y_test, return_counts=True)\n\n# Create a dictionary to see the distribution of each class\ndis_test = dict(zip(unique, counts))\nprint(dis_test)\n\n{0: 163, 1: 2, 2: 24, 3: 11}\n\n\n\n#TEAM RL\n# Plotting the distribution\nplt.figure(figsize=(8, 6))\nplt.bar(dis_train.keys(), dis_train.values())\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.title('Distribution of Classes in Array')\nplt.show()\n\n\n\n\n\n#TEAM RL\n# Plotting the distribution\nplt.figure(figsize=(8, 6))\nplt.bar(dis_test.keys(), dis_test.values())\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.title('Distribution of Classes in Array')\nplt.show()\n\n\n\n\n\n#TEAM RL\n#This code reshapes the training data\nX_train = X_train.reshape(-1,1,30,43)\n\n\n#TEAM RL\n#This code displays the shape of the data\nX_train.shape\n\n(796, 1, 30, 43)\n\n\n\n#TEAM RL\n#This code reshapes the test data\nX_test = X_test.reshape(-1,1,30,43)\n\n\n#TEAM RL\n#This code displays the test shape\nX_test.shape\n\n(200, 1, 30, 43)"
  },
  {
    "objectID": "CNN_New_NoIdl.html#feed-the-data-through-dataloader-to-create-a-batch-of-16-to-input-data-into-the-model-the-model",
    "href": "CNN_New_NoIdl.html#feed-the-data-through-dataloader-to-create-a-batch-of-16-to-input-data-into-the-model-the-model",
    "title": "Import Libraries and required classes and Methods from Idlman",
    "section": "Feed the data through dataloader to create a batch of 16 to input data into the model the model",
    "text": "Feed the data through dataloader to create a batch of 16 to input data into the model the model\n\n#TEAM RL\n#Defines the number of batches\n#Converts the results into pandas so we can evaluate the results.\nB=16\ntrain_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\ntest_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\ntraining_loader = DataLoader(train_dataset,batch_size=B, shuffle=True)\ntesting_loader = DataLoader(test_dataset)\n\n\n#Team RL\n#This code checks the shape of the train data.\nX_train[0].shape\n\n(1, 30, 43)"
  },
  {
    "objectID": "CNN_New_NoIdl.html#base-model",
    "href": "CNN_New_NoIdl.html#base-model",
    "title": "Import Libraries and required classes and Methods from Idlman",
    "section": "Base model",
    "text": "Base model\n\n#TEAM RL\n#How many values are in the input? We use this to help determine the size of subsequent layers\nH=30\nW=43\nD = H*W #30 days * 18 features images\n#How many channels are in the input?\nC = 1\n#How many classes are there?\nclasses = 4\n#How many filters should we use\nfilters = 16\n#how large should our filters be?\nK = 3\n#for comparison, lets define a linear model of similar complexity\nmodel_linear = nn.Sequential(\n  nn.Flatten(), #(B, C, W, H) -&gt; (B, C*W*H) = (B,D)\n  nn.Linear(D, 256),\n  nn.Tanh(),\n  nn.Linear(256, classes),\n)\n\n#A simple convolutional network:\nmodel_cnn = nn.Sequential(\n  #Conv2d follows the pattern of:\n  #Conv2d(# of input channels, #filters/output-channels, #filter-size)\n  nn.Conv2d(C, filters, K, padding=K//2), #$x \\circledast G$\n  nn.Tanh(),#Activation functions work on any size tensorr\n  nn.Flatten(), #Convert from (B, C, W, H) -&gt;(B, D). This way we can use a Linear layer after\n  nn.Linear(filters*D, classes),\n)\n\n\n#TEAM RL\n#This code set sthe device to access the GPU\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\n#TEAM RL\n#This code imports the accuracy score and f1_score from the sklearn metrics.\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n\n#TEAM RL\n#This code defines the loss function as Cross Enthropy Loss\n#This cude defines the results\nloss_func = nn.CrossEntropyLoss()\ncnn_results = train_network(model_cnn, loss_func, training_loader, test_loader=testing_loader, score_funcs={'Acc': accuracy_score,'F1': f1_score}, device=device, epochs=20)\nfc_results = train_network(model_linear, loss_func, training_loader, test_loader=testing_loader, score_funcs={'Acc': accuracy_score,'F1': f1_score}, device=device, epochs=20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code compares the accuracy between the F1 and accuracy of the test data by epoch\nsns.lineplot(x='epoch', y='test Acc', data=cnn_results, label='CNN')\nsns.lineplot(x='epoch', y='test Acc', data=fc_results, label='Fully Conected')\n\n\n\n&lt;Axes: xlabel='epoch', ylabel='test Acc'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code compares the Acc test and train accuracy by epoch\nsns.lineplot(x='epoch', y='test F1', data=cnn_results, label='CNN')\nsns.lineplot(x='epoch', y='test F1', data=fc_results, label='Fully Conected')\n\n&lt;Axes: xlabel='epoch', ylabel='test F1'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code compares accuracy by epock for test and train f1\nsns.lineplot(x='epoch', y='test Acc', data=cnn_results, label='Acc')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results, label='F1')\n\n&lt;Axes: xlabel='epoch', ylabel='test Acc'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code compares the training loss by epoch\nsns.lineplot(x='epoch', y='train F1', data=cnn_results, label='train F1')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results, label='test F1')\n\n&lt;Axes: xlabel='epoch', ylabel='train F1'&gt;"
  },
  {
    "objectID": "CNN_New_NoIdl.html#test-schedulers",
    "href": "CNN_New_NoIdl.html#test-schedulers",
    "title": "Import Libraries and required classes and Methods from Idlman",
    "section": "Test Schedulers",
    "text": "Test Schedulers\n\n#TEAM RL\n#This code defines the learning rate and number of epochs\neta_0 = 0.1\nepochs = 30\n\n\n#TEAM RL\n#This code runs the linear model for the results\n#This code sets the target learning rate\n#This code re-randomizes the weights\n#This code sets the deccay rate\n\nmodel_cnn.apply(weight_reset)#re-randomize the weights of our model so that we don't need to define it again\n\neta_min = 0.0001 #Our desired final learning rate $\\eta_{\\mathit{min}}$\n\ngamma_expo = (eta_min/eta_0)**(1/epochs)#compute $\\gamma$ that results in $\\eta_{\\mathit{min}}$\n\noptimizer = torch.optim.SGD(model_cnn.parameters(), lr=eta_0) #Set up the optimizer\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma_expo)#pick a schedule and pass the optimizer in\n#train like normal and pass along the desired optimizer and schedule\ncnn_results_expolr = train_network(model_cnn, loss_func, training_loader, test_loader=testing_loader, epochs=epochs, optimizer=optimizer, lr_schedule=scheduler, score_funcs={'F1': f1_score}, device=device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code plots the loss for the train and test models\nsns.lineplot(x='epoch', y='test loss', data=cnn_results_expolr, label='test loss')\nsns.lineplot(x='epoch', y='train loss', data=cnn_results_expolr, label='train loss')\n\n&lt;Axes: xlabel='epoch', ylabel='test loss'&gt;\n\n\n\n\n\n\n#Team RL\n#This code resets the weights, and runs the CNN LR Scheduler with SGD model\nmodel_cnn.apply(weight_reset)\n\noptimizer = torch.optim.SGD(model_cnn.parameters(), lr=eta_0)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, epochs//5, gamma=0.003)#I'm telling it to step down by a factor of $\\gamma$ every epochs/4, so this will happen 4 times total.\ncnn_results_Steplr = train_network(model_cnn, loss_func, training_loader, test_loader=testing_loader, epochs=epochs, optimizer=optimizer, lr_schedule=scheduler, score_funcs={'F1': f1_score}, device=device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code runs the cosine LR model\nmodel_cnn.apply(weight_reset)\n\noptimizer = torch.optim.SGD(model_cnn.parameters(), lr=eta_0)\n#Telling the cosine to go down/up/down (thats 3), if we were doing more than 10 epochs I would push this higher\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs//5, eta_min=0.0001)\ncnn_results_Cosinelr = train_network(model_cnn, loss_func, training_loader, test_loader=testing_loader, epochs=epochs, optimizer=optimizer, lr_schedule=scheduler, score_funcs={'F1': f1_score}, device=device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_cnn.apply(weight_reset) #Resetting the weights again so we don't have to define a new model.\n\n#create a training and validation sub-set, since we do not have an explicit validation and test set\n#create a training and validation sub-set, since we do not have an explicit validation and test set\n\nX_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n\ntrain_dataset2 = TensorDataset(torch.tensor(X_train2, dtype=torch.float32), torch.tensor(y_train2, dtype=torch.long))\nval_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\nB=16\n\n#create loaders for the train and validation sub-sets.\ntrain_sub_loader = DataLoader(train_dataset2,batch_size=B, shuffle=True)\nval_sub_loader = DataLoader(val_dataset,batch_size=B)\n\n#our test loader stays the same, never alter or peek on your test data!\n\noptimizer = torch.optim.SGD(model_cnn.parameters(), lr=eta_0)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.63, patience=10)#Set up our plateau schedule using gamma=0.63\ncnn_results_plateau = train_network(model_cnn, loss_func, train_sub_loader, val_loader=val_sub_loader, test_loader=testing_loader, epochs=epochs, optimizer=optimizer, lr_schedule=scheduler, score_funcs={'F1': f1_score}, device=device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code compares the different models\n\nsns.lineplot(x='epoch', y='test F1', data=cnn_results, label='SGD')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_expolr, label='+Exponential Decay')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_Steplr, label='+StepLR')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_Cosinelr, label='+CosineAnnealingLR')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_plateau, label='+Plateau')\n\n&lt;Axes: xlabel='epoch', ylabel='test F1'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code displays the results of the cnn model\n\ncnn_results_expolr.tail()\n\n\n  \n    \n\n\n\n\n\n\nepoch\ntotal time\ntrain loss\ntest loss\ntrain F1\ntest F1\n\n\n\n\n25\n25\n4.190199\n0.302817\n0.298959\n0.942222\n0.943738\n\n\n26\n26\n4.331918\n0.295143\n0.279831\n0.942177\n0.960207\n\n\n27\n27\n4.458435\n0.291527\n0.284929\n0.947560\n0.941396\n\n\n28\n28\n4.583456\n0.286605\n0.277731\n0.945345\n0.941396\n\n\n29\n29\n4.710191\n0.284222\n0.272961\n0.945371\n0.950666\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n#TEAM RL\n#This code compares the Steplr and the expolr\n\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_Steplr, label='+StepLR')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_expolr, label='+Exponential Decay')\n\n&lt;Axes: xlabel='epoch', ylabel='test F1'&gt;"
  },
  {
    "objectID": "CNN_New_NoIdl.html#test-optimizers",
    "href": "CNN_New_NoIdl.html#test-optimizers",
    "title": "Import Libraries and required classes and Methods from Idlman",
    "section": "Test Optimizers",
    "text": "Test Optimizers\n\n#TEAM RL\n#This code sets up and runs the cnn with momentum\n\nmodel_cnn.apply(weight_reset)\n\noptimizer = torch.optim.SGD(model_cnn.parameters(), lr=eta_0, momentum=0.9, nesterov=False)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma_expo)#pick a schedule and pass the optimizer in\n\ncnn_results_momentum = train_network(model_cnn, loss_func, training_loader, test_loader=testing_loader, epochs=epochs, optimizer=optimizer, lr_schedule=scheduler, score_funcs={'Acc': accuracy_score,'F1': f1_score}, device=device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code runs the CNN with nesterov momentum\n\nmodel_cnn.apply(weight_reset)\n\noptimizer = torch.optim.SGD(model_cnn.parameters(), lr=eta_0, momentum=0.9, nesterov=True)\n\ncnn_results_nesterov = train_network(model_cnn, loss_func, training_loader, test_loader=testing_loader, epochs=epochs, optimizer=optimizer, lr_schedule=scheduler, score_funcs={'Acc': accuracy_score,'F1': f1_score}, device=device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code runs the cnn with adamW\n\nmodel_cnn.apply(weight_reset)\n\n#We don't set the learning rate for Adam because it's default is the one you should probably\n#always use, and it can be more sensitive to large changes in learning rate\noptimizer = torch.optim.AdamW(model_cnn.parameters())\ncnn_results_adamW = train_network(model_cnn, loss_func, training_loader, test_loader=testing_loader, epochs=epochs, optimizer=optimizer, lr_schedule=scheduler, score_funcs={'Acc': accuracy_score,'F1': f1_score}, device=device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#Using the same model, for the gradient clipping use if the gradient is greater than 5 or -5 replace the value with 5 or -5\n\nmodel_cnn.apply(weight_reset)\n\nfor p in model_cnn.parameters(): # this does $\\operatorname{clip}_5(\\boldsymbol{g})$\n    p.register_hook(lambda grad: torch.clamp(grad, -5, 5))\n\noptimizer = torch.optim.AdamW(model_cnn.parameters())\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma_expo)#pick a schedule and pass the optimizer in\n\ncnn_results_adamW_clamp= train_network(model_cnn, loss_func, train_sub_loader,val_loader=val_sub_loader , test_loader=testing_loader, epochs=30, optimizer=optimizer,  lr_schedule=scheduler, score_funcs={'F1': accuracy_score}, device=device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code compares the different models.\n\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_plateau, label='SGD')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_momentum, label='SGD w/ Momentum')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_nesterov, label='SGD w/ Nestrov Momentum')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_adamW, label='AdamW')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_adamW_clamp, label='AdamW Clamp')\n\n&lt;Axes: xlabel='epoch', ylabel='test F1'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code compares the adamW with adamW_clamp\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_adamW, label='AdamW')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results_adamW_clamp, label='AdamW Clamp')\n\n&lt;Axes: xlabel='epoch', ylabel='test F1'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code displays the details of the adamW_clamp\ncnn_results_adamW_clamp.tail()\n\n\n  \n    \n\n\n\n\n\n\nepoch\ntotal time\ntrain loss\nval loss\ntest loss\ntrain F1\nval F1\ntest F1\n\n\n\n\n25\n25\n4.034371\n0.065678\n0.093338\n0.100571\n0.962264\n0.9375\n0.945\n\n\n26\n26\n4.195682\n0.065393\n0.093465\n0.101286\n0.966981\n0.9375\n0.945\n\n\n27\n27\n4.362438\n0.065450\n0.093508\n0.101460\n0.965409\n0.9375\n0.945\n\n\n28\n28\n4.543689\n0.065554\n0.093465\n0.101390\n0.966981\n0.9375\n0.945\n\n\n29\n29\n4.769520\n0.065130\n0.093379\n0.101201\n0.965409\n0.9375\n0.945\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n#TEAM RL\n#This code displays the details of the adamW\ncnn_results_adamW.tail()\n\n\n  \n    \n\n\n\n\n\n\nepoch\ntotal time\ntrain loss\ntest loss\ntrain Acc\ntest Acc\ntrain F1\ntest F1\n\n\n\n\n25\n25\n5.058127\n0.067356\n0.162878\n0.972362\n0.940\n0.972609\n0.940769\n\n\n26\n26\n5.225862\n0.055310\n0.072850\n0.979899\n0.975\n0.980022\n0.974614\n\n\n27\n27\n5.380888\n0.092264\n0.260281\n0.968593\n0.925\n0.968509\n0.932634\n\n\n28\n28\n5.550775\n0.071164\n0.119609\n0.972362\n0.960\n0.972332\n0.963890\n\n\n29\n29\n5.718290\n0.053431\n0.120249\n0.973618\n0.970\n0.973734\n0.964554\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n###Set Parameters for Optuna\n\n#TEAM RL\n#Set parameters to optimize hyperparameters\n#How many channels are in the input?\nC = 1\n#How many classes are there?\nclasses = 4\n# How many variables\nH=30\nW=43\nD = H*W #30 days * 43 features images\n#How many samples per batch?\nB = 16\nepochs = 30\nhidden_nodes = 30\n\neta_min = 0.0001 #Our desired final learning rate $\\eta_{\\mathit{min}}$\n\ngamma_expo = (eta_min/eta_0)**(1/epochs)#compute $\\gamma$ that results in $\\eta_{\\mathit{min}}$\n\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\n#TEAM RL\n# define the model with parameters to be optimized , layers, neurons and learning rate\ndef objective(trial):\n\n    X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n\n    train_dataset2 = TensorDataset(torch.tensor(X_train2, dtype=torch.float32), torch.tensor(y_train2, dtype=torch.long))\n    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n    B=16\n\n    #create loaders for the train and validation sub-sets.\n    t_loader = DataLoader(train_dataset2,batch_size=B, shuffle=True)\n    v_loader = DataLoader(val_dataset,batch_size=B)\n\n   #Hidden layer size\n    filters = trial.suggest_int('filters', 16, 32)\n    #K = trial.suggest_int('kernel', 3, 6)\n    #filters = 16\n    K = 3\n    layers = trial.suggest_int('hidden_layers', 1, 6)\n    #How many channels are in the input?\n    C = 1\n    #How many classes are there?\n    classes = 4\n\n    #At least one hidden layer, that take in D inputs\n    sequential_layers = [\n        nn.Conv2d(C, filters, K, padding=K//2),\n        nn.Tanh(),\n    ]\n    #Now lets add in a variable number of hidden layers, depending on what Optuna gave us for the \"layers\" parameter\n    for _ in range(layers-1):\n        sequential_layers.append( nn.Conv2d(filters, filters, K, padding=K//2) )\n        sequential_layers.append( nn.Tanh() )\n\n    #Output layer\n    sequential_layers.append(nn.Flatten())\n    sequential_layers.append(nn.Linear(filters*(H)*(W), classes))\n\n\n    #Now turn the list of layers into a PyTorch Sequential Module\n    cnn_model_op = nn.Sequential(*sequential_layers)\n    #What should our global learning rate be? Notice that we can ask for new hyper-parameters from optuna whenever we want.\n    eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n\n\n    for p in model_cnn.parameters(): # this does $\\operatorname{clip}_5(\\boldsymbol{g})$\n        p.register_hook(lambda grad: torch.clamp(grad, -5, 5))\n\n    optimizer = torch.optim.AdamW(cnn_model_op.parameters())\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma_expo)\n\n    results = train_network(cnn_model_op, loss_func, t_loader, test_loader=v_loader,\n                                     epochs=10, optimizer=optimizer, lr_schedule=scheduler,\n                                     score_funcs={'F1': f1_score}, device=device,\n                                     disable_tqdm=True)\n\n    return results['test F1'].iloc[-1]  # A objective value linked with the Trial object.\n\n\n#TEAM RL\n# Install Optuna\n!pip install optuna\n\nCollecting optuna\n  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\nCollecting alembic&gt;=1.5.0 (from optuna)\n  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\nCollecting colorlog (from optuna)\n  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy&gt;=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nCollecting Mako (from alembic&gt;=1.5.0-&gt;optuna)\n  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: typing-extensions&gt;=4 in /usr/local/lib/python3.10/dist-packages (from alembic&gt;=1.5.0-&gt;optuna) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy&gt;=1.4.2-&gt;optuna) (3.1.1)\nRequirement already satisfied: MarkupSafe&gt;=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako-&gt;alembic&gt;=1.5.0-&gt;optuna) (3.0.2)\nDownloading optuna-4.1.0-py3-none-any.whl (364 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 364.4/364.4 kB 17.2 MB/s eta 0:00:00\nDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.5/233.5 kB 20.7 MB/s eta 0:00:00\nDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\nDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 7.6 MB/s eta 0:00:00\nInstalling collected packages: Mako, colorlog, alembic, optuna\nSuccessfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n\n\n\n#TEAM RL\n# Import optuna\nimport optuna\n\n\n#TEAM RL\n#This code defines the learning rate, the desired learning rate, and the number of epochs\neta_0 = 0.1\nepochs = 30\neta_min = 0.0001 #Our desired final learning rate $\\eta_{\\mathit{min}}$\n\nloss_func = nn.CrossEntropyLoss()\n\n\n#TEAM RL\n#This code defines the study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=75) #Normally we would do more like 50-100 trials,\n#but we are doing less to make sure this notebook runs in a reasonable amount of time\n\n[I 2024-11-18 16:28:26,871] A new study created in memory with name: no-name-c9e23c9e-153f-42f4-9abd-8640931a0e89\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:29,664] Trial 0 finished with value: 0.9546807504917391 and parameters: {'filters': 26, 'hidden_layers': 5, 'learning_rate': 0.005171425369246362}. Best is trial 0 with value: 0.9546807504917391.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:32,773] Trial 1 finished with value: 0.9581403643546972 and parameters: {'filters': 28, 'hidden_layers': 3, 'learning_rate': 0.00020576311923548715}. Best is trial 1 with value: 0.9581403643546972.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:35,529] Trial 2 finished with value: 0.9411090130699838 and parameters: {'filters': 17, 'hidden_layers': 4, 'learning_rate': 0.005887286152040251}. Best is trial 1 with value: 0.9581403643546972.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:38,727] Trial 3 finished with value: 0.9609011627906977 and parameters: {'filters': 25, 'hidden_layers': 6, 'learning_rate': 0.0006071492839431914}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:40,829] Trial 4 finished with value: 0.9609011627906977 and parameters: {'filters': 18, 'hidden_layers': 2, 'learning_rate': 1.5735517528301824e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:43,879] Trial 5 finished with value: 0.9417672413793104 and parameters: {'filters': 29, 'hidden_layers': 6, 'learning_rate': 0.0023154220502613334}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:46,210] Trial 6 finished with value: 0.9501786129182157 and parameters: {'filters': 26, 'hidden_layers': 3, 'learning_rate': 0.0009846645476056252}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:48,237] Trial 7 finished with value: 0.9581403643546972 and parameters: {'filters': 19, 'hidden_layers': 1, 'learning_rate': 0.00866206411648123}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:50,311] Trial 8 finished with value: 0.9434201926384589 and parameters: {'filters': 29, 'hidden_layers': 2, 'learning_rate': 0.0002866431159803829}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:53,243] Trial 9 finished with value: 0.9546807504917391 and parameters: {'filters': 30, 'hidden_layers': 6, 'learning_rate': 0.0012753229408864891}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:56,064] Trial 10 finished with value: 0.9609011627906977 and parameters: {'filters': 22, 'hidden_layers': 5, 'learning_rate': 6.19750966496952e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:28:58,559] Trial 11 finished with value: 0.9488603977531815 and parameters: {'filters': 22, 'hidden_layers': 1, 'learning_rate': 1.1224076636130892e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:01,419] Trial 12 finished with value: 0.9488603977531815 and parameters: {'filters': 16, 'hidden_layers': 2, 'learning_rate': 1.076301637785815e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:05,835] Trial 13 finished with value: 0.9511429901463796 and parameters: {'filters': 21, 'hidden_layers': 4, 'learning_rate': 5.2409068301942585e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:08,391] Trial 14 finished with value: 0.9411090130699838 and parameters: {'filters': 32, 'hidden_layers': 2, 'learning_rate': 7.765727662704764e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:11,262] Trial 15 finished with value: 0.9503558400781804 and parameters: {'filters': 24, 'hidden_layers': 5, 'learning_rate': 0.0007645140167303439}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:13,655] Trial 16 finished with value: 0.9411090130699838 and parameters: {'filters': 19, 'hidden_layers': 3, 'learning_rate': 2.8897500216707175e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:17,324] Trial 17 finished with value: 0.940761689291101 and parameters: {'filters': 25, 'hidden_layers': 4, 'learning_rate': 0.00019559990753318724}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:20,100] Trial 18 finished with value: 0.9488603977531815 and parameters: {'filters': 20, 'hidden_layers': 2, 'learning_rate': 0.0005076796050874489}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:23,549] Trial 19 finished with value: 0.9354411764705881 and parameters: {'filters': 23, 'hidden_layers': 6, 'learning_rate': 0.00011705347158462055}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:25,738] Trial 20 finished with value: 0.9354411764705881 and parameters: {'filters': 18, 'hidden_layers': 1, 'learning_rate': 2.155882534380306e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:28,960] Trial 21 finished with value: 0.9448387014834381 and parameters: {'filters': 22, 'hidden_layers': 5, 'learning_rate': 3.8265804161481634e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:32,433] Trial 22 finished with value: 0.9545046585573262 and parameters: {'filters': 24, 'hidden_layers': 5, 'learning_rate': 0.00010589100610797862}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:35,481] Trial 23 finished with value: 0.9545046585573262 and parameters: {'filters': 21, 'hidden_layers': 6, 'learning_rate': 1.697843197044029e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:38,382] Trial 24 finished with value: 0.9354411764705881 and parameters: {'filters': 27, 'hidden_layers': 5, 'learning_rate': 4.792680616292653e-05}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:41,619] Trial 25 finished with value: 0.9432967752600392 and parameters: {'filters': 16, 'hidden_layers': 6, 'learning_rate': 0.0006049169212690391}. Best is trial 3 with value: 0.9609011627906977.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:44,637] Trial 26 finished with value: 0.9649302080382715 and parameters: {'filters': 23, 'hidden_layers': 4, 'learning_rate': 0.0018781459527602402}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:47,245] Trial 27 finished with value: 0.9609011627906977 and parameters: {'filters': 24, 'hidden_layers': 3, 'learning_rate': 0.001984590908002413}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:50,342] Trial 28 finished with value: 0.9609011627906977 and parameters: {'filters': 26, 'hidden_layers': 4, 'learning_rate': 0.0028714945270377906}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:53,244] Trial 29 finished with value: 0.9545046585573262 and parameters: {'filters': 25, 'hidden_layers': 4, 'learning_rate': 0.00430709394889137}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:55,359] Trial 30 finished with value: 0.9411090130699838 and parameters: {'filters': 18, 'hidden_layers': 2, 'learning_rate': 0.00048583228899018997}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:29:57,957] Trial 31 finished with value: 0.9411090130699838 and parameters: {'filters': 22, 'hidden_layers': 5, 'learning_rate': 0.0014622540824898486}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:00,762] Trial 32 finished with value: 0.9545046585573262 and parameters: {'filters': 23, 'hidden_layers': 5, 'learning_rate': 0.00018929896935410037}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:03,655] Trial 33 finished with value: 0.9532081749049428 and parameters: {'filters': 20, 'hidden_layers': 5, 'learning_rate': 0.00032776209867555205}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:06,594] Trial 34 finished with value: 0.9605913702690378 and parameters: {'filters': 27, 'hidden_layers': 4, 'learning_rate': 1.8303837586508137e-05}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:09,596] Trial 35 finished with value: 0.9574273518769788 and parameters: {'filters': 23, 'hidden_layers': 6, 'learning_rate': 0.0038392462968081}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:12,436] Trial 36 finished with value: 0.9411090130699838 and parameters: {'filters': 21, 'hidden_layers': 3, 'learning_rate': 7.643671389880964e-05}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:14,949] Trial 37 finished with value: 0.9432967752600392 and parameters: {'filters': 25, 'hidden_layers': 3, 'learning_rate': 0.008861374341131212}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:17,837] Trial 38 finished with value: 0.9417672413793104 and parameters: {'filters': 17, 'hidden_layers': 6, 'learning_rate': 0.0009382150335010226}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:20,836] Trial 39 finished with value: 0.9411090130699838 and parameters: {'filters': 28, 'hidden_layers': 4, 'learning_rate': 0.0003469831660921412}. Best is trial 26 with value: 0.9649302080382715.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:23,776] Trial 40 finished with value: 0.9651074423480084 and parameters: {'filters': 20, 'hidden_layers': 5, 'learning_rate': 0.0014809916429001852}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:26,689] Trial 41 finished with value: 0.9286734068627451 and parameters: {'filters': 19, 'hidden_layers': 5, 'learning_rate': 0.0014857489875745183}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:29,997] Trial 42 finished with value: 0.954657335907336 and parameters: {'filters': 20, 'hidden_layers': 5, 'learning_rate': 0.0022246085633305882}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:33,108] Trial 43 finished with value: 0.9609011627906977 and parameters: {'filters': 18, 'hidden_layers': 6, 'learning_rate': 0.0011808361480503745}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:35,655] Trial 44 finished with value: 0.9488603977531815 and parameters: {'filters': 22, 'hidden_layers': 4, 'learning_rate': 0.0007112574332119056}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:37,775] Trial 45 finished with value: 0.9272000483792935 and parameters: {'filters': 19, 'hidden_layers': 1, 'learning_rate': 0.00620283565930791}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:40,464] Trial 46 finished with value: 0.9609011627906977 and parameters: {'filters': 17, 'hidden_layers': 5, 'learning_rate': 1.3869735085234318e-05}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:43,958] Trial 47 finished with value: 0.9552562345242306 and parameters: {'filters': 21, 'hidden_layers': 6, 'learning_rate': 2.773215102494035e-05}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:46,648] Trial 48 finished with value: 0.9545046585573262 and parameters: {'filters': 23, 'hidden_layers': 3, 'learning_rate': 0.0027774111715742353}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:49,373] Trial 49 finished with value: 0.9411090130699838 and parameters: {'filters': 20, 'hidden_layers': 4, 'learning_rate': 0.00024486163575249104}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:52,362] Trial 50 finished with value: 0.9501786129182157 and parameters: {'filters': 26, 'hidden_layers': 4, 'learning_rate': 0.000896077130000277}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:54,950] Trial 51 finished with value: 0.9354411764705881 and parameters: {'filters': 24, 'hidden_layers': 2, 'learning_rate': 0.0018782751946028206}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:57,369] Trial 52 finished with value: 0.9490103543660287 and parameters: {'filters': 24, 'hidden_layers': 3, 'learning_rate': 0.0019787165578810167}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:30:59,753] Trial 53 finished with value: 0.9545046585573262 and parameters: {'filters': 25, 'hidden_layers': 3, 'learning_rate': 0.0030440328950155798}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:02,012] Trial 54 finished with value: 0.9609011627906977 and parameters: {'filters': 22, 'hidden_layers': 2, 'learning_rate': 0.00014554918972276883}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:04,298] Trial 55 finished with value: 0.9430435258092738 and parameters: {'filters': 24, 'hidden_layers': 1, 'learning_rate': 0.0011442731863269176}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:06,877] Trial 56 finished with value: 0.9545046585573262 and parameters: {'filters': 23, 'hidden_layers': 3, 'learning_rate': 0.0003989353784910877}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:09,872] Trial 57 finished with value: 0.9546807504917391 and parameters: {'filters': 32, 'hidden_layers': 5, 'learning_rate': 0.0016286310026850574}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:13,182] Trial 58 finished with value: 0.9496918402777779 and parameters: {'filters': 21, 'hidden_layers': 5, 'learning_rate': 6.77029903483215e-05}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:15,566] Trial 59 finished with value: 0.9439162922113289 and parameters: {'filters': 27, 'hidden_layers': 2, 'learning_rate': 0.0006669408534498382}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:18,338] Trial 60 finished with value: 0.9419340276059962 and parameters: {'filters': 25, 'hidden_layers': 4, 'learning_rate': 0.007025242212875157}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:20,979] Trial 61 finished with value: 0.9474358974358974 and parameters: {'filters': 26, 'hidden_layers': 4, 'learning_rate': 0.0029038383454845123}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:23,866] Trial 62 finished with value: 0.9411090130699838 and parameters: {'filters': 28, 'hidden_layers': 4, 'learning_rate': 0.0034388378315324602}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:26,641] Trial 63 finished with value: 0.9587786869133618 and parameters: {'filters': 29, 'hidden_layers': 4, 'learning_rate': 0.005030597880451986}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:29,053] Trial 64 finished with value: 0.9609011627906977 and parameters: {'filters': 26, 'hidden_layers': 3, 'learning_rate': 0.0022843510687924787}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:32,111] Trial 65 finished with value: 0.9474358974358974 and parameters: {'filters': 19, 'hidden_layers': 6, 'learning_rate': 0.0005359806876715862}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:34,921] Trial 66 finished with value: 0.9474358974358974 and parameters: {'filters': 25, 'hidden_layers': 5, 'learning_rate': 0.004468558367742104}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:37,873] Trial 67 finished with value: 0.9488603977531815 and parameters: {'filters': 23, 'hidden_layers': 4, 'learning_rate': 3.516851222690183e-05}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:40,949] Trial 68 finished with value: 0.9411090130699838 and parameters: {'filters': 20, 'hidden_layers': 5, 'learning_rate': 1.0272717251667033e-05}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:43,519] Trial 69 finished with value: 0.9609011627906977 and parameters: {'filters': 22, 'hidden_layers': 3, 'learning_rate': 0.0008705410032921407}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:47,203] Trial 70 finished with value: 0.9355490890688258 and parameters: {'filters': 27, 'hidden_layers': 6, 'learning_rate': 0.002652939794939412}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:50,332] Trial 71 finished with value: 0.9468805704099822 and parameters: {'filters': 16, 'hidden_layers': 6, 'learning_rate': 0.0011929517025716925}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:53,318] Trial 72 finished with value: 0.9432014013118664 and parameters: {'filters': 18, 'hidden_layers': 6, 'learning_rate': 0.0011696689130724699}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:56,090] Trial 73 finished with value: 0.9609011627906977 and parameters: {'filters': 18, 'hidden_layers': 6, 'learning_rate': 0.0018093020848292027}. Best is trial 40 with value: 0.9651074423480084.\n&lt;ipython-input-64-78223d0e53e8&gt;:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  eta_global = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n[I 2024-11-18 16:31:59,001] Trial 74 finished with value: 0.9301332720588235 and parameters: {'filters': 17, 'hidden_layers': 6, 'learning_rate': 0.0013656051309352561}. Best is trial 40 with value: 0.9651074423480084.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL1\n#print the best values after optuna run with 15 trials.\nprint(study.best_params)\n\n{'filters': 20, 'hidden_layers': 5, 'learning_rate': 0.0014809916429001852}"
  },
  {
    "objectID": "CNN_New_NoIdl.html#create-optimized-model",
    "href": "CNN_New_NoIdl.html#create-optimized-model",
    "title": "Import Libraries and required classes and Methods from Idlman",
    "section": "Create Optimized model",
    "text": "Create Optimized model\n\n#TEAM RL\n#This code prints the Tanh activation function\nprint(nn.Tanh)\n\n&lt;class 'torch.nn.modules.activation.Tanh'&gt;\n\n\n\n#TEAM RL\n#This code defines the deminsions of the data and the optuna model.\n\nH=30\nW=43\nD = H*W #30 days * 18 features images\n#How many channels are in the input?\nC = 1\n#How many classes are there?\nclasses = 4\n#How many filters should we use\nfilters = 20\n#how large should our filters be?\nK = 3\nepochs = 30\n\n\neta_0 = 0.0014809916429001852\neta_min = 0.0014809916429001852/10 #Our desired final learning rate $\\eta_{\\mathit{min}}$\n\ngamma_expo = (eta_min/eta_0)**(1/epochs)#compute $\\gamma$ that results in $\\eta_{\\mathit{min}}$\n\nmodel_cnn_optuna = nn.Sequential(\n    nn.Conv2d(C, filters, 3, padding=3//2),\n    nn.Tanh(),  # This is correct\n    nn.Conv2d(filters, filters, 3, padding=3//2),\n    nn.Tanh(),  # This is correct\n    nn.Conv2d(filters, filters, 3, padding=3//2),\n    nn.Tanh(),  # Changed to create an instance\n    nn.Conv2d(filters, filters, 3, padding=3//2),\n    nn.Tanh(),  # Changed to create an instance\n    nn.Conv2d(filters, filters, 3, padding=3//2),\n    nn.Tanh(),  # Changed to create an instance\n    nn.Flatten(),\n    nn.Linear(filters*(H)*(W), classes),\n)\n\nloss_func = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.AdamW(model_cnn_optuna.parameters())\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma_expo)\n\ncnn_optimal_results = train_network(model_cnn_optuna, loss_func, training_loader, test_loader=testing_loader,optimizer=optimizer, lr_schedule=scheduler,\n score_funcs={'Acc': accuracy_score,'F1': f1_score}, device=device, epochs=30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code graphs the different cnn and optuna models\nsns.lineplot(x='epoch', y='test Acc', data=cnn_results, label=' Acc CNN')\nsns.lineplot(x='epoch', y='test F1', data=cnn_results, label='F1 CNN')\nsns.lineplot(x='epoch', y='test Acc', data=cnn_optimal_results, label='Acc Optuna ')\nsns.lineplot(x='epoch', y='test F1', data=cnn_optimal_results, label='F1 Optuna ')\n\n&lt;Axes: xlabel='epoch', ylabel='test Acc'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code charts the acc and f1 for optuna\nsns.lineplot(x='epoch', y='test Acc', data=cnn_optimal_results, label='Acc Optuna ')\nsns.lineplot(x='epoch', y='test F1', data=cnn_optimal_results, label='F1 Optuna ')\n\n&lt;Axes: xlabel='epoch', ylabel='test Acc'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code gives details on the cnn model\ncnn_optimal_results.tail()\n\n\n  \n    \n\n\n\n\n\n\nepoch\ntotal time\ntrain loss\ntest loss\ntrain Acc\ntest Acc\ntrain F1\ntest F1\n\n\n\n\n25\n25\n6.358382\n0.052744\n0.120286\n0.974874\n0.935\n0.974721\n0.941175\n\n\n26\n26\n6.634184\n0.062742\n0.067561\n0.964824\n0.975\n0.964895\n0.977958\n\n\n27\n27\n6.878587\n0.057404\n0.055161\n0.968593\n0.975\n0.968791\n0.977958\n\n\n28\n28\n7.118338\n0.060437\n0.087960\n0.971106\n0.945\n0.970896\n0.947772\n\n\n29\n29\n7.314496\n0.051773\n0.090873\n0.973618\n0.970\n0.973689\n0.974690\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n##ReLU\n\n#TEAM RL\n#This code defines the deminsions of the ReLU model\n\nH=30\nW=43\nD = H*W #30 days * 18 features images\n#How many channels are in the input?\nC = 1\n#How many classes are there?\nclasses = 4\n#How many filters should we use\nfilters = 20\n#how large should our filters be?\nK = 3\nepochs = 30\n\n\neta_0 = 0.0014809916429001852\neta_min = 0.0014809916429001852/10 #Our desired final learning rate $\\eta_{\\mathit{min}}$\n\ngamma_expo = (eta_min/eta_0)**(1/epochs)#compute $\\gamma$ that results in $\\eta_{\\mathit{min}}$\n\nmodel_cnn_optuna_ReLU = nn.Sequential(\n    nn.Conv2d(C, filters, 3, padding=3//2),\n    nn.ReLU(),  # This is correct\n    nn.Conv2d(filters, filters, 3, padding=3//2),\n    nn.ReLU(),  # This is correct\n    nn.Conv2d(filters, filters, 3, padding=3//2),\n    nn.ReLU(),  # Changed to create an instance\n    nn.Conv2d(filters, filters, 3, padding=3//2),\n    nn.ReLU(),  # Changed to create an instance\n    nn.Conv2d(filters, filters, 3, padding=3//2),\n    nn.ReLU(),  # Changed to create an instance\n    nn.Flatten(),\n    nn.Linear(filters*(H)*(W), classes),\n)\n\nloss_func = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.AdamW(model_cnn_optuna_ReLU.parameters())\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma_expo)\n\ncnn_optimal_ReLU_results = train_network(model_cnn_optuna_ReLU, loss_func, training_loader, test_loader=testing_loader,optimizer=optimizer, lr_schedule=scheduler,\n score_funcs={'Acc': accuracy_score,'F1': f1_score}, device=device, epochs=30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code graphs the ReLU vs the CNN\nsns.lineplot(x='epoch', y='test F1', data=cnn_optimal_results, label='F1 Optuna ')\nsns.lineplot(x='epoch', y='test F1', data=cnn_optimal_ReLU_results, label='F1 Optuna ReLU')\n\n&lt;Axes: xlabel='epoch', ylabel='test F1'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code gives details on the ReLU model\ncnn_optimal_ReLU_results.tail()\n\n\n  \n    \n\n\n\n\n\n\nepoch\ntotal time\ntrain loss\ntest loss\ntrain Acc\ntest Acc\ntrain F1\ntest F1\n\n\n\n\n25\n25\n6.204065\n0.027265\n0.038954\n0.991206\n0.985\n0.991317\n0.984970\n\n\n26\n26\n6.445700\n0.029200\n0.054254\n0.992462\n0.980\n0.992462\n0.983099\n\n\n27\n27\n6.641439\n0.030370\n0.087284\n0.989950\n0.970\n0.989847\n0.974919\n\n\n28\n28\n6.825395\n0.028705\n0.069433\n0.988693\n0.975\n0.988637\n0.978220\n\n\n29\n29\n7.030365\n0.027595\n0.064390\n0.989950\n0.980\n0.989933\n0.983099\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n#Improve the CNN\n\n#TEAM RL\n#This code defines the dimensions of the CNN\n\nH=30\nW=43\nD = H*W #30 days * 18 features images\n#How many channels are in the input?\nC = 1\n#How many classes are there?\nclasses = 4\n#How many filters should we use\nn_filters = 20\n#how large should our filters be?\nK = 3\nepochs = 30\n\n\neta_0 = 0.0014809916429001852\neta_min = 0.0014809916429001852/10 #Our desired final learning rate $\\eta_{\\mathit{min}}$\n\ngamma_expo = (eta_min/eta_0)**(1/epochs)#compute $\\gamma$ that results in $\\eta_{\\mathit{min}}$\n\n\n#TEAM RL\n#This code defines the leak rate\nleak_rate = 0.1 #How much I want the LeakyReLU to \"leak\" by. Anything in [0.01, 0.3] would have been fine.\n\n\n#TEAM RL\n#This code defines the cnnLayer\n\ndef cnnLayer(in_filters, out_filters=None, kernel_size=3):\n    \"\"\"\n    in_filters: how many channels are coming into the layer\n    out_filters: how many channels this layer should learn / output, or `None` if we want to have the same number of channels as the input.\n    kernel_size: how large the kernel should be\n    \"\"\"\n    if out_filters is None:\n        out_filters = in_filters #This is a common pattern, so lets automate it as a default if not asked\n    padding=kernel_size//2 #padding to stay the same size\n    return nn.Sequential( # Combine the layer and activation into a single unit\n        nn.Conv2d(in_filters, out_filters, kernel_size, padding=padding),\n        nn.LeakyReLU(leak_rate)\n    )\n\n\n#TEAM RL\n#This code sets the max pool function and the cnn relu max pool model\ncnn_relu_opt_MaxPool = nn.Sequential(\n    cnnLayer(C, n_filters), cnnLayer(n_filters), cnnLayer(n_filters),\n    nn.MaxPool2d((2,2)),\n    cnnLayer(n_filters, 2*n_filters), cnnLayer(2*n_filters), cnnLayer(2*n_filters),\n    nn.MaxPool2d((2,2)),\n    cnnLayer(2*n_filters, 4*n_filters), cnnLayer(4*n_filters),\n    nn.Flatten(),\n    nn.Linear(4*n_filters*(H//4)*(W//4), classes),\n)\n#Caption: This is our generic CNN code block. Ignoring the object name, this code block can be re-purposed for many different styles of CNN hidden layers by changing the definition of the cnnLayer function.\n\n\n#TEAM RL\n#This code sets up and runs the cnn relu max pool model\nloss_func = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.AdamW(cnn_relu_opt_MaxPool.parameters())\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma_expo)\n\ncnn_relu_opt_MaxPool_results = train_network(cnn_relu_opt_MaxPool, loss_func, training_loader, test_loader=testing_loader,optimizer=optimizer, lr_schedule=scheduler, epochs=30, score_funcs={'Acc': accuracy_score,'F1': f1_score}, device=device)\ndel cnn_relu_opt_MaxPool\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TEAM RL\n#This code displays chargs of the different cnn relu and leakyrelu models\n\nsns.lineplot(x='epoch', y='test Acc', data=cnn_optimal_ReLU_results, label='CNN optuna ReLu')\nsns.lineplot(x='epoch', y='test F1', data=cnn_optimal_ReLU_results, label='CNN optuna ReLU')\nsns.lineplot(x='epoch', y='test Acc', data=cnn_relu_opt_MaxPool_results, label='CNN pool Reluleak')\nsns.lineplot(x='epoch', y='test F1', data=cnn_relu_opt_MaxPool_results, label='CNN pool ReLUleak')\n\n&lt;Axes: xlabel='epoch', ylabel='test Acc'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code plots the CNN pool and ReLU Leak\nsns.lineplot(x='epoch', y='test Acc', data=cnn_relu_opt_MaxPool_results, label=' Acc CNN pool Reluleak')\nsns.lineplot(x='epoch', y='test F1', data=cnn_relu_opt_MaxPool_results, label='F1 CNN pool ReLUleak')\n\n&lt;Axes: xlabel='epoch', ylabel='test Acc'&gt;\n\n\n\n\n\n\n#TEAM RL\n#This code displays the detais of the CNN relu max pool\ncnn_relu_opt_MaxPool_results.tail()\n\n\n  \n    \n\n\n\n\n\n\nepoch\ntotal time\ntrain loss\ntest loss\ntrain Acc\ntest Acc\ntrain F1\ntest F1\n\n\n\n\n25\n25\n8.807555\n0.015569\n0.028819\n0.992462\n0.980\n0.992495\n0.981742\n\n\n26\n26\n9.140236\n0.018442\n0.044599\n0.992462\n0.985\n0.992495\n0.986648\n\n\n27\n27\n9.410333\n0.023547\n0.048751\n0.991206\n0.980\n0.991263\n0.981742\n\n\n28\n28\n9.758967\n0.014698\n0.014070\n0.996231\n0.995\n0.996256\n0.995492\n\n\n29\n29\n10.040505\n0.013251\n0.022630\n0.994975\n0.990\n0.995018\n0.991636"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#approach-machine-learning-supervised-linear-regression",
    "href": "Forecast_vacc_12monthslag.html#approach-machine-learning-supervised-linear-regression",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "Approach: Machine Learning Supervised Linear Regression",
    "text": "Approach: Machine Learning Supervised Linear Regression"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#step-1-import-libraries",
    "href": "Forecast_vacc_12monthslag.html#step-1-import-libraries",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 1: Import Libraries",
    "text": "STEP 1: Import Libraries\n\n#Importing nececcary libraries\nimport numpy as np\nimport pandas as pd\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\nsns.set(color_codes=True)\n\n# filter warnings\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#workflow-data-set",
    "href": "Forecast_vacc_12monthslag.html#workflow-data-set",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": " WORKFLOW: DATA SET ",
    "text": "WORKFLOW: DATA SET"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#step-2-data-description-and-load-the-data",
    "href": "Forecast_vacc_12monthslag.html#step-2-data-description-and-load-the-data",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 2: Data description and Load the Data",
    "text": "STEP 2: Data description and Load the Data\nDataset was retrieved from https://data.chhs.ca.gov/dataset/vaccine-progress-dashboard\n\n#Dataset location\ncovid_vaccines = 'covid19vaccineCleaned.csv'\n\n\n#Using Pandas DataFrame to load the data\n\ndf = pd.read_csv(covid_vaccines)\n\n\n#Display the first five rows in the dataset\ndf.head()\n\n\n\n\n\n\n\n\ncounty\nadministered_date\ntotal_doses\ncumulative_total_doses\npfizer_doses\ncumulative_pfizer_doses\nmoderna_doses\ncumulative_moderna_doses\njj_doses\ncumulative_jj_doses\n...\nfully_vaccinated\ncumulative_fully_vaccinated\nat_least_one_dose\ncumulative_at_least_one_dose\nbooster_recip_count\nbivalent_booster_recip_count\ncumulative_booster_recip_count\ncumulative_bivalent_booster_recip_count\nbooster_eligible_population\nbivalent_booster_eligible_population\n\n\n\n\n0\nAlameda\n1/5/20\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1396064\n1396064\n\n\n1\nAlameda\n7/27/20\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1396064\n1396064\n\n\n2\nAlameda\n7/30/20\n0\n2\n0\n0\n0\n2\n0\n0\n...\n0\n0\n0\n2\n0\n0\n0\n0\n1396064\n1396064\n\n\n3\nAlameda\n7/31/20\n0\n2\n0\n0\n0\n2\n0\n0\n...\n0\n0\n0\n2\n0\n0\n0\n0\n1396064\n1396064\n\n\n4\nAlameda\n8/1/20\n0\n2\n0\n0\n0\n2\n0\n0\n...\n0\n0\n0\n2\n0\n0\n0\n0\n1396064\n1396064\n\n\n\n\n5 rows × 22 columns"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#workflow-clean-and-preprocess-the-dataset",
    "href": "Forecast_vacc_12monthslag.html#workflow-clean-and-preprocess-the-dataset",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": " WORKFLOW: Clean and Preprocess the Dataset ",
    "text": "WORKFLOW: Clean and Preprocess the Dataset"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#step-3-clean-the-data",
    "href": "Forecast_vacc_12monthslag.html#step-3-clean-the-data",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 3: Clean the data",
    "text": "STEP 3: Clean the data\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 49737 entries, 0 to 49736\nData columns (total 22 columns):\n #   Column                                   Non-Null Count  Dtype \n---  ------                                   --------------  ----- \n 0   county                                   49737 non-null  object\n 1   administered_date                        49737 non-null  object\n 2   total_doses                              49737 non-null  int64 \n 3   cumulative_total_doses                   49737 non-null  int64 \n 4   pfizer_doses                             49737 non-null  int64 \n 5   cumulative_pfizer_doses                  49737 non-null  int64 \n 6   moderna_doses                            49737 non-null  int64 \n 7   cumulative_moderna_doses                 49737 non-null  int64 \n 8   jj_doses                                 49737 non-null  int64 \n 9   cumulative_jj_doses                      49737 non-null  int64 \n 10  partially_vaccinated                     49737 non-null  int64 \n 11  total_partially_vaccinated               49737 non-null  int64 \n 12  fully_vaccinated                         49737 non-null  int64 \n 13  cumulative_fully_vaccinated              49737 non-null  int64 \n 14  at_least_one_dose                        49737 non-null  int64 \n 15  cumulative_at_least_one_dose             49737 non-null  int64 \n 16  booster_recip_count                      49737 non-null  int64 \n 17  bivalent_booster_recip_count             49737 non-null  int64 \n 18  cumulative_booster_recip_count           49737 non-null  int64 \n 19  cumulative_bivalent_booster_recip_count  49737 non-null  int64 \n 20  booster_eligible_population              49737 non-null  int64 \n 21  bivalent_booster_eligible_population     49737 non-null  int64 \ndtypes: int64(20), object(2)\nmemory usage: 8.3+ MB\n\n\n\n#Checking for missing values\n\ndf.isnull().sum()\n\ncounty                                     0\nadministered_date                          0\ntotal_doses                                0\ncumulative_total_doses                     0\npfizer_doses                               0\ncumulative_pfizer_doses                    0\nmoderna_doses                              0\ncumulative_moderna_doses                   0\njj_doses                                   0\ncumulative_jj_doses                        0\npartially_vaccinated                       0\ntotal_partially_vaccinated                 0\nfully_vaccinated                           0\ncumulative_fully_vaccinated                0\nat_least_one_dose                          0\ncumulative_at_least_one_dose               0\nbooster_recip_count                        0\nbivalent_booster_recip_count               0\ncumulative_booster_recip_count             0\ncumulative_bivalent_booster_recip_count    0\nbooster_eligible_population                0\nbivalent_booster_eligible_population       0\ndtype: int64"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#step-4-data-preparation-and-visualization",
    "href": "Forecast_vacc_12monthslag.html#step-4-data-preparation-and-visualization",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 4: Data Preparation and Visualization",
    "text": "STEP 4: Data Preparation and Visualization\n\ndf1=df[['administered_date','total_doses']]\ndf1.head()\n\n\n\n\n\n\n\n\nadministered_date\ntotal_doses\n\n\n\n\n0\n1/5/20\n0\n\n\n1\n7/27/20\n0\n\n\n2\n7/30/20\n0\n\n\n3\n7/31/20\n0\n\n\n4\n8/1/20\n0\n\n\n\n\n\n\n\n\n# converting date from datatype to datatime datatype\ndf1['administered_date'] = pd.to_datetime(df1['administered_date'])\nprint(df1.info())\ndf1.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 49737 entries, 0 to 49736\nData columns (total 2 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   administered_date  49737 non-null  datetime64[ns]\n 1   total_doses        49737 non-null  int64         \ndtypes: datetime64[ns](1), int64(1)\nmemory usage: 777.3 KB\nNone\n\n\n\n\n\n\n\n\n\nadministered_date\ntotal_doses\n\n\n\n\n0\n2020-01-05\n0\n\n\n1\n2020-07-27\n0\n\n\n2\n2020-07-30\n0\n\n\n3\n2020-07-31\n0\n\n\n4\n2020-08-01\n0\n\n\n\n\n\n\n\n\ndf1.describe()\n\n\n\n\n\n\n\n\ntotal_doses\n\n\n\n\ncount\n49737.000000\n\n\nmean\n1705.813961\n\n\nstd\n5877.405980\n\n\nmin\n0.000000\n\n\n25%\n9.000000\n\n\n50%\n156.000000\n\n\n75%\n1047.000000\n\n\nmax\n140186.000000\n\n\n\n\n\n\n\n\n#convert to the monthly period and then sum the numbers in the period\ndf1['administered_date'] = df1['administered_date'].dt.to_period(\"M\")\nmonthly_vac = df1.groupby('administered_date').sum().reset_index()\nprint(monthly_vac)\n\n   administered_date  total_doses\n0            2020-01            1\n1            2020-07           40\n2            2020-08          959\n3            2020-09         1882\n4            2020-10          845\n5            2020-11          892\n6            2020-12       562577\n7            2021-01      3383990\n8            2021-02      6237303\n9            2021-03     10348677\n10           2021-04     11751587\n11           2021-05      7174430\n12           2021-06      3503310\n13           2021-07      2112490\n14           2021-08      2711819\n15           2021-09      2493298\n16           2021-10      3402441\n17           2021-11      5530503\n18           2021-12      6300675\n19           2022-01      4591432\n20           2022-02      1837788\n21           2022-03      1059795\n22           2022-04      1931623\n23           2022-05      1538555\n24           2022-06      1197803\n25           2022-07      1141857\n26           2022-08       732091\n27           2022-09      1804981\n28           2022-10      2486550\n29           2022-11      1001875\n\n\n\nmonthly_vac['administered_date'] = monthly_vac['administered_date'].dt.to_timestamp()\n\n\nStep 4.1 Visualize actual doses administered vs periods of elevated cases and deaths\n\n\nplt.figure(figsize=(15,5))\nplt.plot(monthly_vac['administered_date'],monthly_vac['total_doses'])\nplt.xlabel('Date')\nplt.ylabel('Number of doses')\nplt.title('Monthly doses Administered')\nplt.grid(color='r', axis = 'x', linestyle='--', linewidth=1)\nplt.show()\n\n\n\n\n\n\n# create a clolumn with the differences on the monthly vaccines\n# to make this data stationary, increase or decrease in doses\nmonthly_vac['doses_diff'] = monthly_vac['total_doses'].diff()\n# drop the months where data is not available\nmonthly_vac = monthly_vac.dropna()\nprint(monthly_vac)\n\n   administered_date  total_doses  doses_diff\n1         2020-07-01           40        39.0\n2         2020-08-01          959       919.0\n3         2020-09-01         1882       923.0\n4         2020-10-01          845     -1037.0\n5         2020-11-01          892        47.0\n6         2020-12-01       562577    561685.0\n7         2021-01-01      3383990   2821413.0\n8         2021-02-01      6237303   2853313.0\n9         2021-03-01     10348677   4111374.0\n10        2021-04-01     11751587   1402910.0\n11        2021-05-01      7174430  -4577157.0\n12        2021-06-01      3503310  -3671120.0\n13        2021-07-01      2112490  -1390820.0\n14        2021-08-01      2711819    599329.0\n15        2021-09-01      2493298   -218521.0\n16        2021-10-01      3402441    909143.0\n17        2021-11-01      5530503   2128062.0\n18        2021-12-01      6300675    770172.0\n19        2022-01-01      4591432  -1709243.0\n20        2022-02-01      1837788  -2753644.0\n21        2022-03-01      1059795   -777993.0\n22        2022-04-01      1931623    871828.0\n23        2022-05-01      1538555   -393068.0\n24        2022-06-01      1197803   -340752.0\n25        2022-07-01      1141857    -55946.0\n26        2022-08-01       732091   -409766.0\n27        2022-09-01      1804981   1072890.0\n28        2022-10-01      2486550    681569.0\n29        2022-11-01      1001875  -1484675.0\n\n\n\n\nSTEP 4.2 Calulate the sales difference (stationarize time series)\n\nplt.figure(figsize=(15,5))\nplt.plot(monthly_vac['administered_date'],monthly_vac['doses_diff'])\nplt.xlabel('Date')\nplt.ylabel('Number of doses')\nplt.title('Monthly Administered doses  Difference')\nplt.show()\n\n\n\n\n\n# dropping off sales and date to deal just with stacionary data\ndf2 = monthly_vac.drop(['administered_date','total_doses'],axis=1)\nprint(df2.head())\nprint(df2.shape)\n\n   doses_diff\n1        39.0\n2       919.0\n3       923.0\n4     -1037.0\n5        47.0\n(29, 1)\n\n\n\n\nSTEP 4.3 create the variables for month lag from 1 to 12\n\n# Preparing the supervised data\nfor i in range(1,13):\n    col_name = 'week_' + str(i)\n    df2[col_name] = df2['doses_diff'].shift(i)\nprint(df2.head())\ndf2 = df2.dropna().reset_index(drop=True)\ndf2.head(29)\n\n   doses_diff  week_1  week_2  week_3  week_4  week_5  week_6  week_7  week_8  \\\n1        39.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n2       919.0    39.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n3       923.0   919.0    39.0     NaN     NaN     NaN     NaN     NaN     NaN   \n4     -1037.0   923.0   919.0    39.0     NaN     NaN     NaN     NaN     NaN   \n5        47.0 -1037.0   923.0   919.0    39.0     NaN     NaN     NaN     NaN   \n\n   week_9  week_10  week_11  week_12  \n1     NaN      NaN      NaN      NaN  \n2     NaN      NaN      NaN      NaN  \n3     NaN      NaN      NaN      NaN  \n4     NaN      NaN      NaN      NaN  \n5     NaN      NaN      NaN      NaN  \n\n\n\n\n\n\n\n\n\ndoses_diff\nweek_1\nweek_2\nweek_3\nweek_4\nweek_5\nweek_6\nweek_7\nweek_8\nweek_9\nweek_10\nweek_11\nweek_12\n\n\n\n\n0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n2853313.0\n2821413.0\n561685.0\n47.0\n-1037.0\n923.0\n919.0\n39.0\n\n\n1\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n2853313.0\n2821413.0\n561685.0\n47.0\n-1037.0\n923.0\n919.0\n\n\n2\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n2853313.0\n2821413.0\n561685.0\n47.0\n-1037.0\n923.0\n\n\n3\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n2853313.0\n2821413.0\n561685.0\n47.0\n-1037.0\n\n\n4\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n2853313.0\n2821413.0\n561685.0\n47.0\n\n\n5\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n2853313.0\n2821413.0\n561685.0\n\n\n6\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n2853313.0\n2821413.0\n\n\n7\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n2853313.0\n\n\n8\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n4111374.0\n\n\n9\n871828.0\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n1402910.0\n\n\n10\n-393068.0\n871828.0\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n-4577157.0\n\n\n11\n-340752.0\n-393068.0\n871828.0\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n-3671120.0\n\n\n12\n-55946.0\n-340752.0\n-393068.0\n871828.0\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n-1390820.0\n\n\n13\n-409766.0\n-55946.0\n-340752.0\n-393068.0\n871828.0\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n599329.0\n\n\n14\n1072890.0\n-409766.0\n-55946.0\n-340752.0\n-393068.0\n871828.0\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n-218521.0\n\n\n15\n681569.0\n1072890.0\n-409766.0\n-55946.0\n-340752.0\n-393068.0\n871828.0\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n909143.0\n\n\n16\n-1484675.0\n681569.0\n1072890.0\n-409766.0\n-55946.0\n-340752.0\n-393068.0\n871828.0\n-777993.0\n-2753644.0\n-1709243.0\n770172.0\n2128062.0\n\n\n\n\n\n\n\n\ndf2.shape\n\n(17, 13)"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#step-5-split-the-data-in-training-and-test",
    "href": "Forecast_vacc_12monthslag.html#step-5-split-the-data-in-training-and-test",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 5: Split the data in training and test",
    "text": "STEP 5: Split the data in training and test\n\n# split the data between train and test data\ndf2_train = df2.iloc[:-6]\n#print(df2_train.head())\ndf2_test = df2.iloc[-6:]\n#print(df2_test.head())\nprint(\"Train data Shape\", df2_train.shape)\nprint(\"Test data Shape\", df2_test.shape)\n\nTrain data Shape (11, 13)\nTest data Shape (6, 13)\n\n\n\nSTEP 5.1 Transform the data in smaller scale\n\n#scale the values between -1 and 1\nscaler =MinMaxScaler(feature_range=(-1,1))\nscaler.fit(df2_train)\ndf2_train = scaler.transform(df2_train)\ndf2_test = scaler.transform(df2_test)\n\n\n\nSTEP 5.2 Get the X and Y variables fot training and test data\n\nX_train, Y_train = df2_train[:,1:], df2_train[:,0:1]\nX_test, Y_test = df2_test[:,1:], df2_test[:,0:1]\nY_train = Y_train.ravel()\nY_test = Y_test.ravel()\nprint('X_train Shape', X_train.shape)\nprint('Y_train Shape', Y_train.shape)\nprint('X_test Shape', X_test.shape)\nprint('Y_test Shape', Y_test.shape)\n\nX_train Shape (11, 12)\nY_train Shape (11,)\nX_test Shape (6, 12)\nY_test Shape (6,)\n\n\n\n# Make a prediction data frame to merge the predicted\ndoses_dates = monthly_vac['administered_date'][-6:].reset_index(drop=True)\n\n\npredict_df = pd.DataFrame(doses_dates)\npredict_df.head()\n\n\n\n\n\n\n\n\nadministered_date\n\n\n\n\n0\n2022-06-01\n\n\n1\n2022-07-01\n\n\n2\n2022-08-01\n\n\n3\n2022-09-01\n\n\n4\n2022-10-01\n\n\n\n\n\n\n\n\nact_doses = monthly_vac['total_doses'][-7:].to_list()\nprint(act_doses)\n\n[1538555, 1197803, 1141857, 732091, 1804981, 2486550, 1001875]"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#step-6-build-and-and-train-the-model",
    "href": "Forecast_vacc_12monthslag.html#step-6-build-and-and-train-the-model",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 6: Build and and train the Model",
    "text": "STEP 6: Build and and train the Model\n\n# Create the linear regression model and predicted output\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\nlr_pre = model.predict(X_test)\n\n\nSTEP 6.1 Transform the data back to original scale\n\n# transform the values back to the original scale\nlr_pre = lr_pre.reshape(-1,1)\n# Create a matrix with the X test and predicted doses\nlr_pre_test_set = np.concatenate([lr_pre, X_test], axis=1)\nlr_pre_test_set = scaler.inverse_transform(lr_pre_test_set)\n\n\nresult_list = []\nfor i2 in range(0, len(lr_pre_test_set)):\n    result_list.append(lr_pre_test_set[i2][0] + act_doses[i2])\nlr_pre_series = pd.Series(result_list, name = 'Linear Prediction')\npredict_df = predict_df.merge(lr_pre_series, left_index = True, right_index = True)\nprint(predict_df.shape)\nprint(monthly_vac.shape)\npredict_df\n\n(6, 2)\n(29, 3)\n\n\n\n\n\n\n\n\n\nadministered_date\nLinear Prediction\n\n\n\n\n0\n2022-06-01\n9.121751e+05\n\n\n1\n2022-07-01\n1.345304e+06\n\n\n2\n2022-08-01\n2.185924e+06\n\n\n3\n2022-09-01\n-2.875892e+05\n\n\n4\n2022-10-01\n2.787490e+05\n\n\n5\n2022-11-01\n2.186271e+06"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#step-7-model-evaluation",
    "href": "Forecast_vacc_12monthslag.html#step-7-model-evaluation",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 7: Model Evaluation",
    "text": "STEP 7: Model Evaluation\n\n# Evaluation of the model : Calculate the the meanSquared error, MAE and R2\nlr_mse = np.sqrt(mean_squared_error(predict_df['Linear Prediction'], monthly_vac['total_doses'][-6:]))\nlr_mae = mean_absolute_error(predict_df['Linear Prediction'],monthly_vac['total_doses'][-6:])\nlr_r2 = r2_score(predict_df['Linear Prediction'],monthly_vac['total_doses'][-6:])\nprint('Linear regression MSE', lr_mse )\nprint('Linear regression MAE', lr_mae )\nprint('Linear regression R2', lr_r2 )\n\nLinear regression MSE 1465869.1299511846\nLinear regression MAE 1237945.9093991949\nLinear regression R2 -1.55074773726906"
  },
  {
    "objectID": "Forecast_vacc_12monthslag.html#step-8-visualization-of-predictions",
    "href": "Forecast_vacc_12monthslag.html#step-8-visualization-of-predictions",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 8: Visualization of predictions",
    "text": "STEP 8: Visualization of predictions\n\n#Visualize the prediction\nplt.figure(figsize=(15,5))\n# actual sales\nplt.plot(monthly_vac['administered_date'], monthly_vac['total_doses'])\n# predicted sales\nplt.plot(predict_df['administered_date'],predict_df['Linear Prediction'])\nplt.title(\"Future vaccine requirements using Linear Regression\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Doses Required\")\nplt.legend(['Actual Doses', 'Predicted Doses'])\nplt.show()"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#approach-machine-learning-supervised-linear-regression",
    "href": "Forecast_vacc_4weekslag.html#approach-machine-learning-supervised-linear-regression",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "Approach: Machine Learning Supervised Linear Regression",
    "text": "Approach: Machine Learning Supervised Linear Regression"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#step-1-import-libraries",
    "href": "Forecast_vacc_4weekslag.html#step-1-import-libraries",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 1: Import Libraries",
    "text": "STEP 1: Import Libraries\n\n#Importing nececcary libraries\nimport numpy as np\nimport pandas as pd\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\nsns.set(color_codes=True)\n\n# filter warnings\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#workflow-data-set",
    "href": "Forecast_vacc_4weekslag.html#workflow-data-set",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": " WORKFLOW: DATA SET ",
    "text": "WORKFLOW: DATA SET"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#step-2-data-description-and-load-the-data",
    "href": "Forecast_vacc_4weekslag.html#step-2-data-description-and-load-the-data",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 2: Data description and Load the Data",
    "text": "STEP 2: Data description and Load the Data\nDataset was retrieved from https://data.chhs.ca.gov/dataset/vaccine-progress-dashboard\n\n#Dataset location\ncovid_vaccines = 'covid19vaccineCleaned.csv'\n\n\n#Using Pandas DataFrame to load the data\n\ndf = pd.read_csv(covid_vaccines)\n\n\n#Display the first five rows in the dataset\ndf.head()\n\n\n\n\n\n\n\n\ncounty\nadministered_date\ntotal_doses\ncumulative_total_doses\npfizer_doses\ncumulative_pfizer_doses\nmoderna_doses\ncumulative_moderna_doses\njj_doses\ncumulative_jj_doses\n...\nfully_vaccinated\ncumulative_fully_vaccinated\nat_least_one_dose\ncumulative_at_least_one_dose\nbooster_recip_count\nbivalent_booster_recip_count\ncumulative_booster_recip_count\ncumulative_bivalent_booster_recip_count\nbooster_eligible_population\nbivalent_booster_eligible_population\n\n\n\n\n0\nAlameda\n1/5/20\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1396064\n1396064\n\n\n1\nAlameda\n7/27/20\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1396064\n1396064\n\n\n2\nAlameda\n7/30/20\n0\n2\n0\n0\n0\n2\n0\n0\n...\n0\n0\n0\n2\n0\n0\n0\n0\n1396064\n1396064\n\n\n3\nAlameda\n7/31/20\n0\n2\n0\n0\n0\n2\n0\n0\n...\n0\n0\n0\n2\n0\n0\n0\n0\n1396064\n1396064\n\n\n4\nAlameda\n8/1/20\n0\n2\n0\n0\n0\n2\n0\n0\n...\n0\n0\n0\n2\n0\n0\n0\n0\n1396064\n1396064\n\n\n\n\n5 rows × 22 columns"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#workflow-clean-and-preprocess-the-dataset",
    "href": "Forecast_vacc_4weekslag.html#workflow-clean-and-preprocess-the-dataset",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": " WORKFLOW: Clean and Preprocess the Dataset ",
    "text": "WORKFLOW: Clean and Preprocess the Dataset"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#step-3-clean-the-data",
    "href": "Forecast_vacc_4weekslag.html#step-3-clean-the-data",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 3: Clean the data",
    "text": "STEP 3: Clean the data\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 49737 entries, 0 to 49736\nData columns (total 22 columns):\n #   Column                                   Non-Null Count  Dtype \n---  ------                                   --------------  ----- \n 0   county                                   49737 non-null  object\n 1   administered_date                        49737 non-null  object\n 2   total_doses                              49737 non-null  int64 \n 3   cumulative_total_doses                   49737 non-null  int64 \n 4   pfizer_doses                             49737 non-null  int64 \n 5   cumulative_pfizer_doses                  49737 non-null  int64 \n 6   moderna_doses                            49737 non-null  int64 \n 7   cumulative_moderna_doses                 49737 non-null  int64 \n 8   jj_doses                                 49737 non-null  int64 \n 9   cumulative_jj_doses                      49737 non-null  int64 \n 10  partially_vaccinated                     49737 non-null  int64 \n 11  total_partially_vaccinated               49737 non-null  int64 \n 12  fully_vaccinated                         49737 non-null  int64 \n 13  cumulative_fully_vaccinated              49737 non-null  int64 \n 14  at_least_one_dose                        49737 non-null  int64 \n 15  cumulative_at_least_one_dose             49737 non-null  int64 \n 16  booster_recip_count                      49737 non-null  int64 \n 17  bivalent_booster_recip_count             49737 non-null  int64 \n 18  cumulative_booster_recip_count           49737 non-null  int64 \n 19  cumulative_bivalent_booster_recip_count  49737 non-null  int64 \n 20  booster_eligible_population              49737 non-null  int64 \n 21  bivalent_booster_eligible_population     49737 non-null  int64 \ndtypes: int64(20), object(2)\nmemory usage: 8.3+ MB\n\n\n\n#Checking for missing values\n\ndf.isnull().sum()\n\ncounty                                     0\nadministered_date                          0\ntotal_doses                                0\ncumulative_total_doses                     0\npfizer_doses                               0\ncumulative_pfizer_doses                    0\nmoderna_doses                              0\ncumulative_moderna_doses                   0\njj_doses                                   0\ncumulative_jj_doses                        0\npartially_vaccinated                       0\ntotal_partially_vaccinated                 0\nfully_vaccinated                           0\ncumulative_fully_vaccinated                0\nat_least_one_dose                          0\ncumulative_at_least_one_dose               0\nbooster_recip_count                        0\nbivalent_booster_recip_count               0\ncumulative_booster_recip_count             0\ncumulative_bivalent_booster_recip_count    0\nbooster_eligible_population                0\nbivalent_booster_eligible_population       0\ndtype: int64"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#step-4-data-preparation-and-visualization",
    "href": "Forecast_vacc_4weekslag.html#step-4-data-preparation-and-visualization",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 4: Data Preparation and Visualization",
    "text": "STEP 4: Data Preparation and Visualization\n\ndf1=df[['administered_date','total_doses']]\ndf1.head()\n\n\n\n\n\n\n\n\nadministered_date\ntotal_doses\n\n\n\n\n0\n1/5/20\n0\n\n\n1\n7/27/20\n0\n\n\n2\n7/30/20\n0\n\n\n3\n7/31/20\n0\n\n\n4\n8/1/20\n0\n\n\n\n\n\n\n\n\n# converting date from datatype to datatime datatype\ndf1['administered_date'] = pd.to_datetime(df1['administered_date'])\nprint(df1.info())\ndf1.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 49737 entries, 0 to 49736\nData columns (total 2 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   administered_date  49737 non-null  datetime64[ns]\n 1   total_doses        49737 non-null  int64         \ndtypes: datetime64[ns](1), int64(1)\nmemory usage: 777.3 KB\nNone\n\n\n\n\n\n\n\n\n\nadministered_date\ntotal_doses\n\n\n\n\n0\n2020-01-05\n0\n\n\n1\n2020-07-27\n0\n\n\n2\n2020-07-30\n0\n\n\n3\n2020-07-31\n0\n\n\n4\n2020-08-01\n0\n\n\n\n\n\n\n\n\ndf1.describe()\n\n\n\n\n\n\n\n\ntotal_doses\n\n\n\n\ncount\n49737.000000\n\n\nmean\n1705.813961\n\n\nstd\n5877.405980\n\n\nmin\n0.000000\n\n\n25%\n9.000000\n\n\n50%\n156.000000\n\n\n75%\n1047.000000\n\n\nmax\n140186.000000\n\n\n\n\n\n\n\n\n#convert to the weekly period and then sum the numbers in the period\ndf1['administered_date'] = df1['administered_date'].dt.to_period(\"W\")\nmonthly_vac = df1.groupby('administered_date').sum().reset_index()\nprint(monthly_vac)\n\n         administered_date  total_doses\n0    2019-12-30/2020-01-05            1\n1    2020-07-27/2020-08-02           60\n2    2020-08-03/2020-08-09          102\n3    2020-08-10/2020-08-16          171\n4    2020-08-17/2020-08-23          227\n..                     ...          ...\n117  2022-10-17/2022-10-23       587833\n118  2022-10-24/2022-10-30       573149\n119  2022-10-31/2022-11-06       499365\n120  2022-11-07/2022-11-13       455067\n121  2022-11-14/2022-11-20       124851\n\n[122 rows x 2 columns]\n\n\n\nmonthly_vac['administered_date'] = monthly_vac['administered_date'].dt.to_timestamp()\n\n\nStep 4.1 Visualize actual doses administered vs periods of elevated cases and deaths\n\n\nplt.figure(figsize=(15,5))\nplt.plot(monthly_vac['administered_date'],monthly_vac['total_doses'])\nplt.xlabel('Date')\nplt.ylabel('Number of doses')\nplt.title('Monthly doses Administered')\nplt.grid(color='r', axis = 'x', linestyle='--', linewidth=1)\nplt.show()\n\n\n\n\n\n\n# create a clolumn with the differences on the monthly vaccines\n# to make this data stationary, increase or decrease in doses\nmonthly_vac['doses_diff'] = monthly_vac['total_doses'].diff()\n# drop the months where data is not available\nmonthly_vac = monthly_vac.dropna()\nprint(monthly_vac)\n\n    administered_date  total_doses  doses_diff\n1          2020-07-27           60        59.0\n2          2020-08-03          102        42.0\n3          2020-08-10          171        69.0\n4          2020-08-17          227        56.0\n5          2020-08-24          337       110.0\n..                ...          ...         ...\n117        2022-10-17       587833     -3115.0\n118        2022-10-24       573149    -14684.0\n119        2022-10-31       499365    -73784.0\n120        2022-11-07       455067    -44298.0\n121        2022-11-14       124851   -330216.0\n\n[121 rows x 3 columns]\n\n\n\n\nSTEP 4.2 Calulate the sales difference (stationarize time series)\n\nplt.figure(figsize=(15,5))\nplt.plot(monthly_vac['administered_date'],monthly_vac['doses_diff'])\nplt.xlabel('Date')\nplt.ylabel('Number of doses')\nplt.title('Weekly Administered doses  Difference')\nplt.show()\n\n\n\n\n\n# dropping off sales and date to deal just with stacionary data\ndf2 = monthly_vac.drop(['administered_date','total_doses'],axis=1)\nprint(df2.head())\nprint(df2.shape)\n\n   doses_diff\n1        59.0\n2        42.0\n3        69.0\n4        56.0\n5       110.0\n(121, 1)\n\n\n\n\nSTEP 4.3 Create the variables for week lag 1, lag 2, lag 3 and lag 4\n\n# Preparing the supervised data\nfor i in range(1,5):\n    col_name = 'week_' + str(i)\n    df2[col_name] = df2['doses_diff'].shift(i)\nprint(df2.head())\ndf2 = df2.dropna().reset_index(drop=True)\ndf2.head(29)\n\n   doses_diff  week_1  week_2  week_3  week_4\n1        59.0     NaN     NaN     NaN     NaN\n2        42.0    59.0     NaN     NaN     NaN\n3        69.0    42.0    59.0     NaN     NaN\n4        56.0    69.0    42.0    59.0     NaN\n5       110.0    56.0    69.0    42.0    59.0\n\n\n\n\n\n\n\n\n\ndoses_diff\nweek_1\nweek_2\nweek_3\nweek_4\n\n\n\n\n0\n110.0\n56.0\n69.0\n42.0\n59.0\n\n\n1\n143.0\n110.0\n56.0\n69.0\n42.0\n\n\n2\n-42.0\n143.0\n110.0\n56.0\n69.0\n\n\n3\n-12.0\n-42.0\n143.0\n110.0\n56.0\n\n\n4\n37.0\n-12.0\n-42.0\n143.0\n110.0\n\n\n5\n-170.0\n37.0\n-12.0\n-42.0\n143.0\n\n\n6\n-63.0\n-170.0\n37.0\n-12.0\n-42.0\n\n\n7\n-22.0\n-63.0\n-170.0\n37.0\n-12.0\n\n\n8\n-34.0\n-22.0\n-63.0\n-170.0\n37.0\n\n\n9\n-10.0\n-34.0\n-22.0\n-63.0\n-170.0\n\n\n10\n12.0\n-10.0\n-34.0\n-22.0\n-63.0\n\n\n11\n47.0\n12.0\n-10.0\n-34.0\n-22.0\n\n\n12\n32.0\n47.0\n12.0\n-10.0\n-34.0\n\n\n13\n-105.0\n32.0\n47.0\n12.0\n-10.0\n\n\n14\n821.0\n-105.0\n32.0\n47.0\n12.0\n\n\n15\n-338.0\n821.0\n-105.0\n32.0\n47.0\n\n\n16\n138545.0\n-338.0\n821.0\n-105.0\n32.0\n\n\n17\n76841.0\n138545.0\n-338.0\n821.0\n-105.0\n\n\n18\n23809.0\n76841.0\n138545.0\n-338.0\n821.0\n\n\n19\n241241.0\n23809.0\n76841.0\n138545.0\n-338.0\n\n\n20\n297952.0\n241241.0\n23809.0\n76841.0\n138545.0\n\n\n21\n180221.0\n297952.0\n241241.0\n23809.0\n76841.0\n\n\n22\n171405.0\n180221.0\n297952.0\n241241.0\n23809.0\n\n\n23\n203478.0\n171405.0\n180221.0\n297952.0\n241241.0\n\n\n24\n178309.0\n203478.0\n171405.0\n180221.0\n297952.0\n\n\n25\n-13102.0\n178309.0\n203478.0\n171405.0\n180221.0\n\n\n26\n392080.0\n-13102.0\n178309.0\n203478.0\n171405.0\n\n\n27\n236586.0\n392080.0\n-13102.0\n178309.0\n203478.0\n\n\n28\n34817.0\n236586.0\n392080.0\n-13102.0\n178309.0\n\n\n\n\n\n\n\n\ndf2.shape\n\n(117, 5)"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#step-5-split-the-data-in-training-and-test",
    "href": "Forecast_vacc_4weekslag.html#step-5-split-the-data-in-training-and-test",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 5: Split the data in training and test",
    "text": "STEP 5: Split the data in training and test\n\n# split the data between train and test data\ndf2_train = df2.iloc[:-41]\n#print(df2_train.head())\ndf2_test = df2.iloc[-41:]\n#print(df2_test.head())\nprint(\"Train data Shape\", df2_train.shape)\nprint(\"Test data Shape\", df2_test.shape)\n\nTrain data Shape (76, 5)\nTest data Shape (41, 5)\n\n\n\nSTEP 5.1 Transform the data in smaller scale\n\n#scale the values between -1 and 1\nscaler =MinMaxScaler(feature_range=(-1,1))\nscaler.fit(df2_train)\ndf2_train = scaler.transform(df2_train)\ndf2_test = scaler.transform(df2_test)\n\n\n\nSTEP 5.2 Get the X and Y variables fot training and test data\n\nX_train, Y_train = df2_train[:,1:], df2_train[:,0:1]\nX_test, Y_test = df2_test[:,1:], df2_test[:,0:1]\nY_train = Y_train.ravel()\nY_test = Y_test.ravel()\nprint('X_train Shape', X_train.shape)\nprint('Y_train Shape', Y_train.shape)\nprint('X_test Shape', X_test.shape)\nprint('Y_test Shape', Y_test.shape)\n\nX_train Shape (76, 4)\nY_train Shape (76,)\nX_test Shape (41, 4)\nY_test Shape (41,)\n\n\n\n# Make a prediction data frame to merge the predicted\ndoses_dates = monthly_vac['administered_date'][-41:].reset_index(drop=True)\n\n\npredict_df = pd.DataFrame(doses_dates)\npredict_df.head()\n\n\n\n\n\n\n\n\nadministered_date\n\n\n\n\n0\n2022-02-07\n\n\n1\n2022-02-14\n\n\n2\n2022-02-21\n\n\n3\n2022-02-28\n\n\n4\n2022-03-07\n\n\n\n\n\n\n\n\nact_doses = monthly_vac['total_doses'][-42:].to_list()\nprint(act_doses)\n\n[646017, 494228, 397874, 352295, 300866, 225824, 208574, 189722, 368029, 495224, 468160, 420129, 393861, 350550, 356018, 370074, 376855, 298748, 293060, 252926, 256406, 240591, 215756, 296781, 299775, 252301, 211658, 174082, 156658, 135105, 94573, 261954, 477769, 546434, 553331, 582720, 590948, 587833, 573149, 499365, 455067, 124851]"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#step-6-build-and-and-train-the-model",
    "href": "Forecast_vacc_4weekslag.html#step-6-build-and-and-train-the-model",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 6: Build and and train the Model",
    "text": "STEP 6: Build and and train the Model\n\n# Create the linear regression model and predicted output\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\nlr_pre = model.predict(X_test)\n\n\nSTEP 6.1 Transform the data back to original scale\n\n# transform the values back to the original scale\nlr_pre = lr_pre.reshape(-1,1)\n# Create a matrix with the X test and predicted doses\nlr_pre_test_set = np.concatenate([lr_pre, X_test], axis=1)\nlr_pre_test_set = scaler.inverse_transform(lr_pre_test_set)\n\n\nresult_list = []\nfor i2 in range(0, len(lr_pre_test_set)):\n    result_list.append(lr_pre_test_set[i2][0] + act_doses[i2])\nlr_pre_series = pd.Series(result_list, name = 'Linear Prediction')\npredict_df = predict_df.merge(lr_pre_series, left_index = True, right_index = True)\nprint(predict_df.shape)\nprint(monthly_vac.shape)\npredict_df\n\n(41, 2)\n(121, 3)\n\n\n\n\n\n\n\n\n\nadministered_date\nLinear Prediction\n\n\n\n\n0\n2022-02-07\n523649.888005\n\n\n1\n2022-02-14\n386046.856277\n\n\n2\n2022-02-21\n312774.787282\n\n\n3\n2022-02-28\n272622.884373\n\n\n4\n2022-03-07\n243233.424820\n\n\n5\n2022-03-14\n181632.881283\n\n\n6\n2022-03-21\n182190.672122\n\n\n7\n2022-03-28\n162469.407557\n\n\n8\n2022-04-04\n384042.069589\n\n\n9\n2022-04-11\n526574.673285\n\n\n10\n2022-04-18\n497577.778346\n\n\n11\n2022-04-25\n461673.766660\n\n\n12\n2022-05-02\n403477.665101\n\n\n13\n2022-05-09\n326749.447007\n\n\n14\n2022-05-16\n340103.545744\n\n\n15\n2022-05-23\n360731.373377\n\n\n16\n2022-05-30\n371581.858831\n\n\n17\n2022-06-06\n286799.437905\n\n\n18\n2022-06-13\n289145.007155\n\n\n19\n2022-06-20\n232605.070898\n\n\n20\n2022-06-27\n238181.655413\n\n\n21\n2022-07-04\n229595.179698\n\n\n22\n2022-07-11\n202342.614511\n\n\n23\n2022-07-18\n308104.189784\n\n\n24\n2022-07-25\n298934.429094\n\n\n25\n2022-08-01\n251382.034338\n\n\n26\n2022-08-08\n214952.457345\n\n\n27\n2022-08-15\n155919.273025\n\n\n28\n2022-08-22\n134487.216267\n\n\n29\n2022-08-29\n115411.697262\n\n\n30\n2022-09-05\n74734.548174\n\n\n31\n2022-09-12\n284084.952258\n\n\n32\n2022-09-19\n521174.404075\n\n\n33\n2022-09-26\n595380.081222\n\n\n34\n2022-10-03\n625243.244168\n\n\n35\n2022-10-10\n639202.606294\n\n\n36\n2022-10-17\n607913.898724\n\n\n37\n2022-10-24\n593323.180715\n\n\n38\n2022-10-31\n576210.927269\n\n\n39\n2022-11-07\n484336.388135\n\n\n40\n2022-11-14\n437323.112329"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#step-7-model-evaluation",
    "href": "Forecast_vacc_4weekslag.html#step-7-model-evaluation",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 7: Model Evaluation",
    "text": "STEP 7: Model Evaluation\n\n# Evaluation of the model : Calculate the the meanSquared error, MAE and R2\nlr_mse = np.sqrt(mean_squared_error(predict_df['Linear Prediction'], monthly_vac['total_doses'][-41:]))\nlr_mae = mean_absolute_error(predict_df['Linear Prediction'],monthly_vac['total_doses'][-41:])\nlr_r2 = r2_score(predict_df['Linear Prediction'],monthly_vac['total_doses'][-41:])\nprint('Linear regression MSE', lr_mse )\nprint('Linear regression MAE', lr_mae )\nprint('Linear regression R2', lr_r2 )\n\nLinear regression MSE 83297.38304324557\nLinear regression MAE 54024.10710838589\nLinear regression R2 0.7114574966795546"
  },
  {
    "objectID": "Forecast_vacc_4weekslag.html#step-8-visualization-of-predictions",
    "href": "Forecast_vacc_4weekslag.html#step-8-visualization-of-predictions",
    "title": "\n Group 2 - Final Project - November 26th, 2022\n\n",
    "section": "STEP 8: Visualization of predictions",
    "text": "STEP 8: Visualization of predictions\n\n#Visualize the prediction\nplt.figure(figsize=(15,5))\n# actual sales\nplt.plot(monthly_vac['administered_date'], monthly_vac['total_doses'])\n# predicted sales\nplt.plot(predict_df['administered_date'],predict_df['Linear Prediction'])\nplt.title(\"Future vaccine requirements using Linear Regression\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Doses Required\")\nplt.legend(['Actual Doses', 'Predicted Doses'])\nplt.show()"
  }
]
---
title: "projects"
image: profile.jpg
about:
  template: jolla
  links:
    - icon: twitter
      text: Twitter
      href: https://twitter.com/pleonato
    - icon: linkedin
      text: LinkedIn
      href: https://linkedin.com/pleonato
    - icon: github
      text: Github
      href: https://github.com/leonatopl
google-analytics: "G-L4WJJ84KT0"
---

### ***Comparing Assets within Technology Adoption: Forecast Framework***

**1. Core Objectives** The primary goal of this project is to determine where emerging technologies specifically Artificial Intelligence (AI) and cryptocurrencies (decentralization) currently sit on the technology adoption lifecycle. The authors aim to forecast the potential prices of these assets, identify good market entry and exit points, and determine if macro-economic factors impact these nascent asset classes differently than traditional mature investments.

**2. Exploratory Data Analysis (EDA) and Market Insights** The authors analyzed a dataset spanning from 2000 to 2025, comparing AI and cryptocurrencies against traditional stocks (like Apple), indexes, metals, and macroeconomic data (like M2 money supply and CPI). Key findings from the EDA include:

-   **Technology Adoption Phase:** Cryptocurrencies are estimated to be starting at the steepest point of the adoption S-curve, with an estimated eight years of exceptional returns remaining before maturity. AI is estimated to be at 35% adoption (the "early majority" phase), with about five years of rapid adoption remaining.

    ![](images/Technology_adoption-02.png)

```{=html}
<!-- -->
```
-   **Macroeconomic Impact:** Unlike mature assets (like traditional stocks), pure-play AI (e.g., NVIDIA) and cryptocurrencies have low or negative correlations to macroeconomic variables like inflation (CPI) and money supply. Their prices are currently driven by rapid adoption rather than macro-economy health.

    ![](images/Sector correlation-01.png)

**3. Forecasting Framework and Modeling** The project evaluates various deep learning architectures (Fully Connected, RNN, LSTM, GRU, CNN, and their bidirectional variants) to forecast Apple stock prices as a baseline model that could eventually be applied to other assets. The forecasting was divided into three time horizons using a sliding window approach:

**Long-term:** 1-year windows predicting 1 year ahead (using data from 2009 onward).

**Medium-term:** 120-day windows predicting 6 months ahead (using data from 2014 onwar)

**Short-term:** 15-day windows predicting 3 weeks ahead (using data from 2020 onward).

**4. Key Model Results** The models were benchmarked against a Naïve forecasting method.

Simpler models (like standard RNN, GRU, and Bidirectional RNN/GRU) consistently outperformed highly complex models like LSTM and Bidirectional LSTM, which proved slow, difficult to tune, and prone to overfitting.

For multivariable long-term forecasting, reducing dimensionality using Principal Component Analysis (PCA) successfully mitigated multicollinearity issues and improved model performance.

Recommending carefully matching the model architecture and preprocessing strategy to the specific forecasting window is more important than simply utilizing the most complex neural network.

### ***Neural Network Recession Prediction Indicator:***

Economic cycles affect the financial well-being of the US and the world population. A downturn can negatively impact the populace. Inflation causes purchasing power to decrease. When unemployment rises, people lose their ability to meet their financial borrowers are unable to stay current.

This is the ideal time for us to create a model to predict the future state of the economy. Economic anomalies exist that make this hard to predict. There are geopolitical factors that have influence over the future of the economy, including the elections in the United States. There is also a wide range of opinions from economists.

Machine learning (ML) models are increasingly replacing traditional statistical models due to their ability to analyze vast amounts of data and multiple variables, resulting in more reliable outcomes.

![](images/Picture8.jpg){fig-align="center"}

Our objective is to predict if a recession is going to happen in the next six months. We have four scenarios that we define for our target variable:no recession in the next six months, mild recession will begin in the next six months, moderate recession will start in the next six months and will last between six and twelve months,and deep recession will start in the next six months. This will last between twelve and thirty-six months.

We assess the normalization of the final 43 selected variables.

![](images/normal.jpg){fig-align="center"}

We consider it critical to have a split records in training and testing where we keep the distribution of the classes. For that, we imported the train_test_split method from the sklearn.model_selection library. This ensures that the distribution of the training and test is the same. We believe that this split is critical to design a robust model. In the above figure, we can see the distribution of the class for the test dataset, in yellow, after the split. We can see the same distribution in a reduced number of records for the test dataset.

![](images/Picture13.jpg){fig-align="center"}

There are three possible models that we could use fully connected (FC), convolutional (CNN), and recurrent neural network (RNN) , even though we understand that recurrent neural networks are the most suitable for serial data, we didn't want to limit our study to one model and work with an approach that evaluated all possible alternatives.

![](images/aproach.jpg){fig-align="center"}

With the CNN, we proceeded with further optimization using maxpooling and different no-saturated activation functions like ReLU and LeakyReLU. We use max-pooling because, during our data exploration, we identify extreme values that might be captured in max-pooling. This layer also reduces the dimension, improving performance and lowering cost. The best results were with the leakyReLU activation function and maxpooling with 99% F1 score.

In this study, we developed a comprehensive machine learning model to predict the likelihood and severity of upcoming recessions. We selected this thesis due to the uncertainty of the current economic environment. Using a 36-month look-back period and 43 economic indicators, our model offers a significant advancement over existing predictive methodologies.

We evaluated FC, CNN, RNN and LSTM models and different window sizes with different slides: 30-day window sizes with a 10-day slide to predict the recession index for the 30th day. Then only for the RNN and LSTM we evaluate the following cases: a 30-day window with a 10-day slide to predict the recession index for the 60th day and a 60-day window with a 30-day slide to predict the recession index for the 90th day. For each neural network model, We developed a simple base model, tried every scheduler and optimizer and selected the best result, and with those, we ran Optuna to find the best hyperparameters with 75 trials. In the following graph we are just showing the Optuna model for all the different combinations.

![](images/Model1.jpg){fig-align="center"}

![](images/Model2.jpg){fig-align="center"}

After evaluating all the models, we identified the need for additional data to develop a model that has a higher number of rolling days and can predict future recession indexes. We got the best result with the CNN, but this one requires a longer processing time, which will increase cost. RNN and LSTM are the best alternatives; although both drop the F1 scores with a limited number of training data, the stability of these models is remarkable. Also, we are concerned about overfitting, and we recommend early stopping and exploring data augmentation with synthetic data generation. There are two possible techniques: SMOTE (Synthetic Minority Over-sampling Technique) and GANs (Generative Adversarial Networks), to create synthetic data that mimics the characteristics of the historical records. The alternative will be to go back in history, although we are concerned that some of the variables might not be available if we go back too many years.

### ***Factors that Influence Health: US Life Expectancy:***

Life expectancy is a vital indicator that provides insight regarding a population's overall health, and it serves as one of the core indicators of calculating the United Nations Human Development Index across countries. According to the World Health Organization data, life expectancy at birth in the United States is 78.5 years for both sexes in 2019; but when compared to other high-income countries, the United States's life expectancy measured to be 2.4 years lower and ranked 40th in the world (World Health Organization, 2019).

![](images/lifeExp_hist.png){fig-align="center"}

![](images/life_exp_byST.jpg){fig-align="center"}

With life expectancy being a primary indicator for a population's overall health, this project aims to determine how lifestyle and socio-economic factors have impacted life expectancy and its disparities among counties (minimum geographical location) within the United States between the years of 2019 to 2023.

![](images/corr.jpg){fig-align="center"}

The data used in our study revealed the impact of location on life expectancy. The South region had poor lifestyle behaviors, as we observed from the graphs. As we learned from the literature review, physical and social environments in which people live helped determine the quality of life and the life expectancy. The South region was more rural, less educated, and had the highest percentage of poverty. Comparatively, the West region had the highest life expectancy and was on the other side of the spectrum of the South region for the variables in the study. We also observed during the exploration of the data the South region is the region with the lowest life expectancy with an average of 76.07, while the West region had an average life expectancy about three years higher, in the range of 79.15.

![](images/byRegion.jpg){fig-align="center"}

![](images/byregion2.jpg){fig-align="center"}

While evaluating the validity of the model, we observed that the residuals were randomly distributed. However, in the QQ plot, we observed tails with high curvature, specifically to the right, which reveals the presence of extreme outliers.

![](images/QQplot.jpg){fig-align="center"}

The Residuals vs Leverage graph did not show the presence of any high leverage points. We calculated the Full Model and best subset model, eliminating the outliers, and assessed the validity of the model again.

The R-squared and adjusted R-squared improve from 0.6851 to 0.7798

Lasso approach used a shrinkage technique that would significantly reduce the variance. Lasso eliminated the same variables that were found in the stepwise approach. Evaluating the MSPE and comparing it with the cross-validation ten-fold for both full model and bestsubset model, we saw that Lasso provided a lower MSPE. 

![](images/Lasso.jpg){fig-align="center"}

To evaluate the health factor that contributes the most to life expectancy, we picked the Lasso model because it has the lowest Error MSPE = 3.027 and is the model that predicts the average life expectancy in the US more accurately. 

![](images/Lasso_results.jpg){fig-align="center"}

### ***Classification of potential Donors and prediction of Gift Amount:***

begin{left}We have been approached by a non-profit organization to build a classification and predictive model that will be used to increase the cost-effectiveness of their current direct marketing campaign. The organization has provided data with a historical background to its direct marketing campaign. Personalized address mailers are sent out to previous donors for \$2.00; the average donation gift amount is \$14.50 from respondents. However, the campaign is operating at a loss due to low donation rates of 10%. The organization would like for us to take a provided dataset of prior donors to create our analytical models in order to make their direct marketing campaign not operate at a loss.

![](fundraising2.jpg){fig-align="center" width="235"}

We assess the potential profit of our study by using the adjusted confusion matrix from the ensemble classification model with the average donation amount from the AutoNeural validation model to calculate various profit-losses, considering with and without opportunity costs. We will then use the percentages of the adjusted confusion matrix to estimate the profit or loss generated by the score data results. We will also consider the lift provided by the score data.

The resulting profit from our model, based on the validation classification data with the score donation value, was \$1,695.65. We ignore the opportunity cost from false negatives in our profit evaluation.

### ***Forecast model to predict the demand of Covit-19 vaccine***

According to the World Health Organization, Covid-19 is an infectious disease that cause symptoms such as fever, cough, tiredness, and loss of taste or smell. The World Health Organization stated that anyone can get sick with the virus and become extremely sick or die at any age. Covid-19 outbreak goes back to late December 2019 in Wuhan, Hubei, China as mentioned by (Wu et al., 2020). Moreover, (Ybarra, 2022) added that some Covid-19 cases suffer from something called long-term Covid which means some Covid-19 symptoms affect parts of the body such as the lungs, causing trouble in breathing, harsh cough, and shortness of breath. More than a year ago, a vaccine was derived against the Covid-19 virus. 4.4 billion people have had one or more doses, which represents 56% of the world's population (Mallapaty et al., 2021). In this project, we will study the vaccination process in California and try to predict the booster-eligible population in all California counties.

![](covid1.JPG){fig-align="center" width="312"}

I worked on a model to calculate the demand for vaccines in the state of California. Two models were presented aggregating the data in monthly buckets with 12-month lags as independent variables. In the second model, the data was aggregated in weekly buckets with 4-week lags as independent variables. The data was split into two datasets without breaking the sequence of the dataset, training with 65% of the data and testing with 35% of the remaining values in the dataset. The model was trained using a linear regression algorithm.

![](for_covid.png){fig-align="center" width="325"}

I recommended the model with weekly buckets to provide accurate predictions of future demand.

### ***Salaries, Foreign born vs Native in Texas.***

In this project, we evaluate the attributes that influence a worker's salary in Texas. We proceed with analytics of the data including data exploration and data cleansing.

![](salary_dis.jpg){fig-align="center" width="281"}

Deep understanding of SCOR model, Lean six-sigma DMAIC methodology, costing/forecasting, process mapping, profitability analysis tools, supply chains, and operations planning. The main attributes evaluated were place of birth, jobs, occupations, income,sex, and educational attainment. For this project we used Google Big Query, Python linear regression, and decision tree in SAS.

![](salary.png){fig-align="center" width="211"}

The best salary among the four males was predicted for a foreign male with a graduate degree who runs his own business.
